{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/250 Loss: 0.2209\n",
      "Epoch 100/250 Loss: 0.0372\n",
      "Epoch 150/250 Loss: 0.0153\n",
      "Epoch 200/250 Loss: 0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 19:11:57.774868: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/250 Loss: 0.0032\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Softmax, LSTM\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (input, response)\n",
    "train_data = [\n",
    "    (\"hello there\", \"hi, how can I help\"),\n",
    "    (\"hi\", \"hello, what can I do\"),\n",
    "    (\"goodbye\", \"goodbye, have a nice day\"),\n",
    "    (\"see you later\", \"see you soon, goodbye\"),\n",
    "    (\"I want to order pizza\", \"sure, what toppings do you want\"),\n",
    "    (\"can I get a burger\", \"what size burger would you like\"),\n",
    "    (\"what is the weather\", \"the weather today is sunny\"),\n",
    "    (\"is it raining\", \"no rain expected today\"),\n",
    "    (\"hey, I want some pasta\", \"what kind of pasta would you prefer\"),\n",
    "    (\"do you have vegetarian options?\", \"yes, we have several vegetarian dishes\"),\n",
    "    (\"good morning\", \"good morning, how may I assist you?\"),\n",
    "    (\"bye\", \"take care, see you later\"),\n",
    "    (\"will it be hot today?\", \"expect warm temperatures all day\"),\n",
    "    (\"can I order a salad?\", \"what dressing would you like on your salad?\"),\n",
    "    (\"thanks, goodbye\", \"you're welcome, goodbye!\"),\n",
    "    (\"tell me the forecast\", \"the forecast shows clear skies\"),\n",
    "    (\"what's your name?\", \"i am your assistant, here to help\"),\n",
    "    (\"can I have a coffee?\", \"sure, would you like it black or with milk?\"),\n",
    "    (\"thank you for the help\", \"happy to assist you anytime\"),\n",
    "    (\"are you open today?\", \"yes, we are open from 9 am to 9 pm\"),\n",
    "    (\"could you help me with my order\", \"of course, what would you like to order\"),\n",
    "    (\"are there any gluten free options\", \"yes, we have several gluten free dishes available\"),\n",
    "    (\"what are today's specials\", \"today's special is grilled salmon with vegetables\"),\n",
    "    (\"how late are you open\", \"we are open until 10 pm tonight\"),\n",
    "    (\"can you recommend a dessert\", \"our chocolate lava cake is very popular\"),\n",
    "    (\"I need to change my order\", \"sure, what changes would you like to make\"),\n",
    "    (\"do you deliver\", \"yes, we deliver within a 5 mile radius\"),\n",
    "    (\"what payment methods do you accept\", \"we accept cash, credit cards, and mobile payments\"),\n",
    "    (\"is there a parking facility\", \"yes, free parking is available behind the restaurant\"),\n",
    "    (\"thank you very much\", \"you're welcome, happy to help\"),\n",
    "    (\"I have a food allergy\", \"please let us know your allergy, and we will accommodate\"),\n",
    "    (\"can I book a table\", \"yes, for how many people and what time\"),\n",
    "    (\"what's your restaurant address\", \"we are located at 123 Main Street\"),\n",
    "    (\"do you have vegan meals\", \"yes, we offer delicious vegan options\"),\n",
    "    (\"can I get nutritional information\", \"nutritional info is available on our website\"),\n",
    "    (\"how long is the wait time\", \"usually about 15 minutes during peak hours\"),\n",
    "    (\"do you have a kids menu\", \"yes, we have a special menu for children\"),\n",
    "    (\"can I cancel my order\", \"please provide your order number to cancel\"),\n",
    "    (\"what are your opening hours\", \"we are open from 9 am to 10 pm daily\"),\n",
    "    (\"is takeout available\", \"yes, you can order takeout anytime during opening hours\"),        \n",
    "]\n",
    "\n",
    "# Build vocabulary\n",
    "all_texts = [t[0] + \" \" + t[1] for t in train_data]\n",
    "all_words = set(word for sentence in all_texts for word in sentence.lower().split())\n",
    "word2idx = {w: i + 1 for i, w in enumerate(sorted(all_words))}\n",
    "idx2word = np.array(['<pad>'] + sorted(all_words))\n",
    "vocab_size = len(idx2word)\n",
    "\n",
    "max_input_len = 6\n",
    "max_resp_len = 8\n",
    "\n",
    "def encode_sentence(sent, max_len):\n",
    "    words = sent.lower().split()\n",
    "    seq = [word2idx.get(w, 0) for w in words]\n",
    "    seq = seq[:max_len] + [0] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "X_input = np.array([encode_sentence(t[0], max_input_len) for t in train_data])\n",
    "X_resp_in = np.array([encode_sentence(t[1], max_resp_len) for t in train_data])\n",
    "X_resp_out = np.array([encode_sentence(t[1], max_resp_len)[1:] + [0] for t in train_data]) # shifted\n",
    "\n",
    "class Expert(Model):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense2 = Dense(d_model)\n",
    "    def call(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "class GatingNetwork(Model):\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super().__init__()\n",
    "        self.dense = Dense(num_experts)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "    def call(self, x):\n",
    "        logits = self.dense(x)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "class MoEResponseGenerator(Model):\n",
    "    def __init__(self, vocab_size, d_model, num_experts, max_resp_len, lstm_units=128):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = [Expert(d_model) for _ in range(num_experts)]\n",
    "        self.gating_net = GatingNetwork(d_model, num_experts)\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        self.to_h = Dense(lstm_units)\n",
    "        self.to_c = Dense(lstm_units)\n",
    "        self.output_layer = Dense(vocab_size)\n",
    "        self.max_resp_len = max_resp_len\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_seq, resp_in_seq = inputs\n",
    "        enc_emb = self.embedding(input_seq)\n",
    "        pooled = self.global_pool(enc_emb)\n",
    "\n",
    "        gating_probs = self.gating_net(pooled)\n",
    "        expert_outs = tf.stack([expert(pooled) for expert in self.experts], axis=1)\n",
    "        gated_rep = tf.reduce_sum(tf.expand_dims(gating_probs, 2) * expert_outs, axis=1)\n",
    "\n",
    "        h_state = self.to_h(gated_rep)\n",
    "        c_state = self.to_c(gated_rep)\n",
    "\n",
    "        resp_emb = self.embedding(resp_in_seq)\n",
    "        lstm_out, _, _ = self.lstm(resp_emb, initial_state=[h_state, c_state])\n",
    "\n",
    "        logits = self.output_layer(lstm_out)\n",
    "        return logits, gating_probs\n",
    "\n",
    "# Hyperparams and dataset\n",
    "d_model = 32\n",
    "num_experts = 4\n",
    "batch_size = 2\n",
    "epochs = 250\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((X_input, X_resp_in), X_resp_out)).shuffle(20).batch(batch_size)\n",
    "\n",
    "model = MoEResponseGenerator(vocab_size, d_model, num_experts, max_resp_len)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, _ = model(inputs, training=True)\n",
    "        loss = loss_fn(labels, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for in_batch, out_batch in dataset:\n",
    "        loss = train_step(in_batch, out_batch)\n",
    "        total_loss += loss.numpy()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Loss: {total_loss/len(dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: hello there\n",
      "Generated response: how can i help\n",
      "Top expert used: 0\n",
      "Gating probabilities: [0.267 0.244 0.252 0.237]\n",
      "---------------------------------\n",
      "Input: I want to order pizza\n",
      "Generated response: what toppings do you want\n",
      "Top expert used: 0\n",
      "Gating probabilities: [0.273 0.25  0.267 0.21 ]\n",
      "---------------------------------\n",
      "Input: goodbye\n",
      "Generated response: have a nice day\n",
      "Top expert used: 0\n",
      "Gating probabilities: [0.321 0.238 0.2   0.241]\n",
      "---------------------------------\n",
      "Input: can you help me order food\n",
      "Generated response: what would like like like\n",
      "Top expert used: 1\n",
      "Gating probabilities: [0.262 0.267 0.245 0.226]\n",
      "---------------------------------\n",
      "Input: hi, what's going on?\n",
      "Generated response: how how i help you?\n",
      "Top expert used: 2\n",
      "Gating probabilities: [0.261 0.228 0.27  0.24 ]\n",
      "---------------------------------\n",
      "Input: will it rain tomorrow?\n",
      "Generated response: warm temperatures all day\n",
      "Top expert used: 2\n",
      "Gating probabilities: [0.275 0.202 0.291 0.233]\n",
      "---------------------------------\n",
      "Input: how do I say goodbye politely?\n",
      "Generated response: have have a allergy, and\n",
      "Top expert used: 0\n",
      "Gating probabilities: [0.275 0.238 0.241 0.246]\n",
      "---------------------------------\n",
      "Input: what toppings do you have?\n",
      "Generated response: we offer delicious vegan options\n",
      "Top expert used: 0\n",
      "Gating probabilities: [0.268 0.254 0.252 0.225]\n",
      "---------------------------------\n",
      "Input: is today sunny or cloudy?\n",
      "Generated response: we offer vegan vegan\n",
      "Top expert used: 0\n",
      "Gating probabilities: [0.334 0.266 0.176 0.224]\n",
      "---------------------------------\n",
      "Input: see you soon\n",
      "Generated response: you soon, goodbye\n",
      "Top expert used: 0\n",
      "Gating probabilities: [0.287 0.26  0.224 0.228]\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "def sample_from_logits(logits, temperature=1.0, top_k=5):\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        values, _ = tf.math.top_k(logits, k=top_k)\n",
    "        min_values = values[:, -1, None]\n",
    "        logits = tf.where(\n",
    "            logits < min_values,\n",
    "            tf.fill(tf.shape(logits), float('-inf')),\n",
    "            logits,\n",
    "        )\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    next_token = tf.random.categorical(tf.math.log(probabilities), num_samples=1)\n",
    "    return tf.squeeze(next_token, axis=-1).numpy()\n",
    "\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    input_seq = np.array([encode_sentence(input_text, max_input_len)])\n",
    "    response_seq = np.zeros((1, max_resp_len), dtype=np.int32)\n",
    "    generated_tokens = []\n",
    "    gating_probs = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        logits, gating_probs = model((input_seq, response_seq), training=False)\n",
    "        logits_step = logits[:, i % max_resp_len, :]\n",
    "        \n",
    "        next_token = sample_from_logits(logits_step, temperature=temperature, top_k=top_k)[0]\n",
    "\n",
    "        if next_token == 0:  # end on padding token\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(idx2word[next_token])\n",
    "        if i + 1 < max_resp_len:\n",
    "            response_seq[0, i] = next_token\n",
    "        else:\n",
    "            response_seq = np.roll(response_seq, -1, axis=1)\n",
    "            response_seq[0, -1] = next_token\n",
    "\n",
    "    top_expert = np.argmax(gating_probs[0].numpy()) if gating_probs is not None else -1\n",
    "    return \" \".join(generated_tokens), top_expert, gating_probs[0].numpy()\n",
    "\n",
    "\n",
    "# Demo\n",
    "inference_prompts = [\n",
    "    \"hello there\",\n",
    "    \"I want to order pizza\",\n",
    "    \"goodbye\",\n",
    "    \"can you help me order food\",\n",
    "    \"hi, what's going on?\",\n",
    "    \"will it rain tomorrow?\",\n",
    "    \"how do I say goodbye politely?\",\n",
    "    \"what toppings do you have?\",\n",
    "    \"is today sunny or cloudy?\",\n",
    "    \"see you soon\"\n",
    "]\n",
    "for input_text in inference_prompts:\n",
    "    response, expert_used, gating_distribution = generate_response(model, input_text, max_len=500)\n",
    "\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Generated response:\", response)\n",
    "    print(\"Top expert used:\", expert_used)\n",
    "    print(\"Gating probabilities:\", np.round(gating_distribution, 3))\n",
    "    print(\"---------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
