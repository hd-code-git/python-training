{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Softmax, LSTM, Conv2D, Flatten, MaxPooling2D, LayerNormalization, Dropout\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# MoE Text Generation Model Components\n",
    "# ---------------------------------------------\n",
    "\n",
    "class Expert(Model):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.dense2 = Dense(d_model)\n",
    "    def call(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "class GatingNetwork(Model):\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super().__init__()\n",
    "        self.dense = Dense(num_experts)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "    def call(self, x):\n",
    "        logits = self.dense(x)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "class MoEResponseGenerator(Model):\n",
    "    def __init__(self, vocab_size, d_model, num_experts, max_resp_len, lstm_units=128):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = [Expert(d_model) for _ in range(num_experts)]\n",
    "        self.gating_net = GatingNetwork(d_model, num_experts)\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        self.to_h = Dense(lstm_units)\n",
    "        self.to_c = Dense(lstm_units)\n",
    "        self.output_layer = Dense(vocab_size)\n",
    "        self.max_resp_len = max_resp_len\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_seq, resp_in_seq = inputs\n",
    "        enc_emb = self.embedding(input_seq)\n",
    "        pooled = self.global_pool(enc_emb)\n",
    "\n",
    "        gating_probs = self.gating_net(pooled)\n",
    "        expert_outs = tf.stack([expert(pooled) for expert in self.experts], axis=1)\n",
    "        gated_rep = tf.reduce_sum(tf.expand_dims(gating_probs, 2) * expert_outs, axis=1)\n",
    "\n",
    "        h_state = self.to_h(gated_rep)\n",
    "        c_state = self.to_c(gated_rep)\n",
    "\n",
    "        resp_emb = self.embedding(resp_in_seq)\n",
    "        lstm_out, _, _ = self.lstm(resp_emb, initial_state=[h_state, c_state])\n",
    "\n",
    "        logits = self.output_layer(lstm_out)\n",
    "        return logits, gating_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Utility functions for text encoding and generation\n",
    "# ---------------------------------------------\n",
    "\n",
    "# (Your train_data list is assumed defined here - omitted for brevity, but same as your list)\n",
    "\n",
    "# Build vocabulary based on train_data (input + responses)\n",
    "train_data = [\n",
    "    (\"hello there\", \"hi, how can I help\"),\n",
    "    (\"hi\", \"hello, what can I do\"),\n",
    "    (\"goodbye\", \"goodbye, have a nice day\"),\n",
    "    (\"see you later\", \"see you soon, goodbye\"),\n",
    "    (\"I want to order pizza\", \"sure, what toppings do you want\"),\n",
    "    (\"can I get a burger\", \"what size burger would you like\"),\n",
    "    (\"what is the weather\", \"the weather today is sunny\"),\n",
    "    (\"is it raining\", \"no rain expected today\"),\n",
    "    (\"hey, I want some pasta\", \"what kind of pasta would you prefer\"),\n",
    "    (\"do you have vegetarian options?\", \"yes, we have several vegetarian dishes\"),\n",
    "    (\"good morning\", \"good morning, how may I assist you?\"),\n",
    "    (\"bye\", \"take care, see you later\"),\n",
    "    (\"will it be hot today?\", \"expect warm temperatures all day\"),\n",
    "    (\"can I order a salad?\", \"what dressing would you like on your salad?\"),\n",
    "    (\"thanks, goodbye\", \"you're welcome, goodbye!\"),\n",
    "    (\"tell me the forecast\", \"the forecast shows clear skies\"),\n",
    "    (\"what's your name?\", \"i am your assistant, here to help\"),\n",
    "    (\"can I have a coffee?\", \"sure, would you like it black or with milk?\"),\n",
    "    (\"thank you for the help\", \"happy to assist you anytime\"),\n",
    "    (\"are you open today?\", \"yes, we are open from 9 am to 9 pm\"),\n",
    "    (\"could you help me with my order\", \"of course, what would you like to order\"),\n",
    "    (\"are there any gluten free options\", \"yes, we have several gluten free dishes available\"),\n",
    "    (\"what are today's specials\", \"today's special is grilled salmon with vegetables\"),\n",
    "    (\"how late are you open\", \"we are open until 10 pm tonight\"),\n",
    "    (\"can you recommend a dessert\", \"our chocolate lava cake is very popular\"),\n",
    "    (\"I need to change my order\", \"sure, what changes would you like to make\"),\n",
    "    (\"do you deliver\", \"yes, we deliver within a 5 mile radius\"),\n",
    "    (\"what payment methods do you accept\", \"we accept cash, credit cards, and mobile payments\"),\n",
    "    (\"is there a parking facility\", \"yes, free parking is available behind the restaurant\"),\n",
    "    (\"thank you very much\", \"you're welcome, happy to help\"),\n",
    "    (\"I have a food allergy\", \"please let us know your allergy, and we will accommodate\"),\n",
    "    (\"can I book a table\", \"yes, for how many people and what time\"),\n",
    "    (\"what's your restaurant address\", \"we are located at 123 Main Street\"),\n",
    "    (\"do you have vegan meals\", \"yes, we offer delicious vegan options\"),\n",
    "    (\"can I get nutritional information\", \"nutritional info is available on our website\"),\n",
    "    (\"how long is the wait time\", \"usually about 15 minutes during peak hours\"),\n",
    "    (\"do you have a kids menu\", \"yes, we have a special menu for children\"),\n",
    "    (\"can I cancel my order\", \"please provide your order number to cancel\"),\n",
    "    (\"what are your opening hours\", \"we are open from 9 am to 10 pm daily\"),\n",
    "    (\"is takeout available\", \"yes, you can order takeout anytime during opening hours\"),        \n",
    "]\n",
    "\n",
    "all_texts = [t[0] + \" \" + t[1] for t in train_data]\n",
    "all_words = set(word for sentence in all_texts for word in sentence.lower().split())\n",
    "word2idx = {w: i + 1 for i, w in enumerate(sorted(all_words))}\n",
    "idx2word = np.array(['<pad>'] + sorted(all_words))\n",
    "vocab_size = len(idx2word)\n",
    "max_input_len = 6\n",
    "max_resp_len = 8\n",
    "\n",
    "def encode_sentence(sent, max_len):\n",
    "    words = sent.lower().split()\n",
    "    seq = [word2idx.get(w, 0) for w in words]\n",
    "    seq = seq[:max_len] + [0] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0, top_k=5):\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        values, _ = tf.math.top_k(logits, k=top_k)\n",
    "        min_values = values[:, -1, None]\n",
    "        logits = tf.where(\n",
    "            logits < min_values,\n",
    "            tf.fill(tf.shape(logits), float('-inf')),\n",
    "            logits,\n",
    "        )\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    next_token = tf.random.categorical(tf.math.log(probabilities), num_samples=1)\n",
    "    return tf.squeeze(next_token, axis=-1).numpy()\n",
    "\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    input_seq = np.array([encode_sentence(input_text, max_input_len)])\n",
    "    response_seq = np.zeros((1, max_resp_len), dtype=np.int32)\n",
    "    generated_tokens = []\n",
    "    gating_probs = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        logits, gating_probs = model((input_seq, response_seq), training=False)\n",
    "        logits_step = logits[:, i % max_resp_len, :]\n",
    "        next_token = sample_from_logits(logits_step, temperature=temperature, top_k=top_k)[0]\n",
    "        if next_token == 0:\n",
    "            break\n",
    "        generated_tokens.append(idx2word[next_token])\n",
    "        if i + 1 < max_resp_len:\n",
    "            response_seq[0, i] = next_token\n",
    "        else:\n",
    "            response_seq = np.roll(response_seq, -1, axis=1)\n",
    "            response_seq[0, -1] = next_token\n",
    "\n",
    "    top_expert = np.argmax(gating_probs[0].numpy()) if gating_probs is not None else -1\n",
    "    return \" \".join(generated_tokens), top_expert, gating_probs[0].numpy()\n",
    "\n",
    "# Build training dataset\n",
    "X_input = np.array([encode_sentence(t[0], max_input_len) for t in train_data])\n",
    "X_resp_in = np.array([encode_sentence(t[1], max_resp_len) for t in train_data])\n",
    "X_resp_out = np.array([encode_sentence(t[1], max_resp_len)[1:] + [0] for t in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# MNIST Digit Classifier Components\n",
    "# ---------------------------------------------\n",
    "\n",
    "class DigitClassifier(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(32, (3,3), activation='relu')\n",
    "        self.pool1 = MaxPooling2D((2,2))\n",
    "        self.conv2 = Conv2D(64, (3,3), activation='relu')\n",
    "        self.pool2 = MaxPooling2D((2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(64, activation='relu')\n",
    "        self.out = Dense(10)  # digits 0-9 logits\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return self.out(x)\n",
    "\n",
    "def prepare_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MNIST digit classifier...\n",
      "Epoch 1/3\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9237 - loss: 0.2573 - val_accuracy: 0.9805 - val_loss: 0.0711\n",
      "Epoch 2/3\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9793 - loss: 0.0685 - val_accuracy: 0.9833 - val_loss: 0.0549\n",
      "Epoch 3/3\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9845 - loss: 0.0499 - val_accuracy: 0.9868 - val_loss: 0.0517\n",
      "Training MoE text generation model...\n",
      "Epoch 50/300: Loss = 0.2517\n",
      "Epoch 100/300: Loss = 0.0390\n",
      "Epoch 150/300: Loss = 0.0130\n",
      "Epoch 200/300: Loss = 0.0055\n",
      "Epoch 250/300: Loss = 0.0028\n",
      "Epoch 300/300: Loss = 0.0015\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Training setup and training loops\n",
    "# ---------------------------------------------\n",
    "\n",
    "digit_classifier = DigitClassifier()\n",
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = prepare_mnist_data()\n",
    "digit_classifier.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "print(\"Training MNIST digit classifier...\")\n",
    "digit_classifier.fit(x_train_mnist, y_train_mnist, epochs=3, batch_size=128, validation_split=0.1)\n",
    "\n",
    "d_model = 32\n",
    "num_experts = 6\n",
    "lstm_units = 128\n",
    "max_epochs = 300\n",
    "batch_size = 2\n",
    "\n",
    "moe_model = MoEResponseGenerator(vocab_size, d_model, num_experts, max_resp_len, lstm_units)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((X_input, X_resp_in), X_resp_out)).shuffle(50).batch(batch_size)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, _ = moe_model(inputs, training=True)\n",
    "        loss = loss_fn(labels, logits)\n",
    "    grads = tape.gradient(loss, moe_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, moe_model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "print(\"Training MoE text generation model...\")\n",
    "for epoch in range(max_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in train_dataset:\n",
    "        loss = train_step(batch[0], batch[1])\n",
    "        total_loss += loss.numpy()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{max_epochs}: Loss = {total_loss / len(train_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Image increment and multimodal handler\n",
    "# ---------------------------------------------\n",
    "\n",
    "def get_incremented_digit_image(input_image, x_dataset, y_dataset):\n",
    "    # Normalize pixel values and ensure float32\n",
    "    img = input_image.astype('float32')\n",
    "    if img.max() > 1.0:\n",
    "        img /= 255.0\n",
    "\n",
    "    # Add channel dimension if missing\n",
    "    if len(img.shape) == 2:   # grayscale image shape (28,28)\n",
    "        img = np.expand_dims(img, axis=-1)  # become (28,28,1)\n",
    "\n",
    "    # Add batch dimension for model input\n",
    "    img = np.expand_dims(img, axis=0)  # become (1,28,28,1)\n",
    "\n",
    "    logits = digit_classifier(img)\n",
    "    pred_digit = tf.argmax(logits, axis=1).numpy()[0]\n",
    "\n",
    "    inc_digit = 0 if pred_digit == 9 else pred_digit + 1\n",
    "    idx = np.where(y_dataset == inc_digit)[0][0]\n",
    "    inc_image = x_dataset[idx][:, :, 0]  # remove channel for display\n",
    "\n",
    "    return pred_digit, inc_digit, inc_image\n",
    "\n",
    "\n",
    "def multimodal_handler(input_text=None, input_image=None):\n",
    "    if input_text:\n",
    "        response, expert, gating_probs = generate_response(moe_model, input_text)\n",
    "        print(f\"Text Input: {input_text}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Top expert used: {expert}\")\n",
    "        print(f\"Gating probs: {np.round(gating_probs, 3)}\")\n",
    "    if input_image is not None:\n",
    "        pred_digit, inc_digit, inc_image = get_incremented_digit_image(input_image, x_test_mnist, np.argmax(y_test_mnist, axis=1))\n",
    "        print(f\"Image input digit: {pred_digit}, incremented output digit: {inc_digit}\")\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title(f\"Input: {pred_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(inc_image, cmap='gray')\n",
    "        plt.title(f\"Output: {inc_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Input: hello, how are you?\n",
      "Response: would would like like black website\n",
      "Top expert used: 0\n",
      "Gating probs: [0.197 0.134 0.153 0.18  0.166 0.17 ]\n",
      "Image input digit: 5, incremented output digit: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Input: What number is this?\n",
      "Response: what can i help\n",
      "Top expert used: 5\n",
      "Gating probs: [0.178 0.154 0.168 0.151 0.171 0.179]\n",
      "Image input digit: 4, incremented output digit: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Demo code to test multimodal system\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Text only\n",
    "multimodal_handler(input_text=\"hello, how are you?\")\n",
    "\n",
    "# Image only (from MNIST test)\n",
    "multimodal_handler(input_image=x_test_mnist[15])\n",
    "\n",
    "# Both\n",
    "multimodal_handler(input_text=\"What number is this?\", input_image=x_test_mnist[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Input: hello there\n",
      "Response: how can i help\n",
      "Top expert used: 0\n",
      "Gating probs: [0.189 0.14  0.168 0.175 0.173 0.154]\n",
      "---------------------------------\n",
      "Text Input: I want to order pizza\n",
      "Response: what toppings do you want\n",
      "Top expert used: 2\n",
      "Gating probs: [0.181 0.144 0.194 0.155 0.162 0.165]\n",
      "---------------------------------\n",
      "Text Input: goodbye\n",
      "Response: have a nice day\n",
      "Top expert used: 0\n",
      "Gating probs: [0.181 0.153 0.154 0.173 0.172 0.167]\n",
      "---------------------------------\n",
      "Text Input: can you help me order food\n",
      "Response: course, what would like to cancel\n",
      "Top expert used: 0\n",
      "Gating probs: [0.179 0.143 0.174 0.173 0.156 0.174]\n",
      "---------------------------------\n",
      "Text Input: hi, what's going on?\n",
      "Response: are your assistant, 9 to help\n",
      "Top expert used: 0\n",
      "Gating probs: [0.183 0.151 0.17  0.16  0.17  0.165]\n",
      "---------------------------------\n",
      "Text Input: will it rain tomorrow?\n",
      "Response: course, assist you anytime\n",
      "Top expert used: 5\n",
      "Gating probs: [0.17  0.139 0.164 0.181 0.159 0.187]\n",
      "---------------------------------\n",
      "Text Input: how do I say goodbye politely?\n",
      "Response: we a a 5 mile\n",
      "Top expert used: 3\n",
      "Gating probs: [0.177 0.153 0.157 0.18  0.162 0.171]\n",
      "---------------------------------\n",
      "Text Input: what toppings do you have?\n",
      "Response: accept cash, from cards, and mobile\n",
      "Top expert used: 5\n",
      "Gating probs: [0.161 0.166 0.145 0.187 0.151 0.19 ]\n",
      "---------------------------------\n",
      "Text Input: is today sunny or cloudy?\n",
      "Response: morning, how may available behind\n",
      "Top expert used: 3\n",
      "Gating probs: [0.193 0.154 0.132 0.228 0.162 0.132]\n",
      "---------------------------------\n",
      "Text Input: see you soon\n",
      "Response: you soon, goodbye\n",
      "Top expert used: 3\n",
      "Gating probs: [0.176 0.15  0.169 0.187 0.147 0.172]\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "greeting_examples = [\n",
    "    \"hello\",\n",
    "    \"hi there\",\n",
    "    \"good morning\",\n",
    "    \"hey, how are you?\",\n",
    "    \"what's up?\"\n",
    "]\n",
    "\n",
    "goodbye_examples = [\n",
    "    \"goodbye\",\n",
    "    \"see you later\",\n",
    "    \"bye for now\",\n",
    "    \"talk to you soon\",\n",
    "    \"have a great day\"\n",
    "]\n",
    "\n",
    "order_food_examples = [\n",
    "    \"I want to order a pizza\",\n",
    "    \"can I get a burger please?\",\n",
    "    \"what sides do you have?\",\n",
    "    \"I'd like a vegetarian pasta\",\n",
    "    \"do you have gluten free options?\"\n",
    "]\n",
    "\n",
    "weather_examples = [\n",
    "    \"what's the weather today?\",\n",
    "    \"will it rain tomorrow?\",\n",
    "    \"is it sunny outside?\",\n",
    "    \"what's the forecast for this week?\",\n",
    "    \"do I need an umbrella today?\"\n",
    "]\n",
    "\n",
    "miscellaneous_examples = [\n",
    "    \"what's your name?\",\n",
    "    \"can you tell me a joke?\",\n",
    "    \"how do I say goodbye politely?\",\n",
    "    \"are you open on weekends?\",\n",
    "    \"what time do you close?\"\n",
    "]\n",
    "\n",
    "inference_prompts = \n",
    "for input_text in inference_prompts:\n",
    "    multimodal_handler(input_text)\n",
    "\n",
    "    # print(\"Input:\", input_text)\n",
    "    # print(\"Generated response:\", response)\n",
    "    # print(\"Top expert used:\", expert_used)\n",
    "    # print(\"Gating probabilities:\", np.round(gating_distribution, 3))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (input, response)\n",
    "train_data = [\n",
    "    (\"hello there\", \"hi, how can I help\"),\n",
    "    (\"hi\", \"hello, what can I do\"),\n",
    "    (\"goodbye\", \"goodbye, have a nice day\"),\n",
    "    (\"see you later\", \"see you soon, goodbye\"),\n",
    "    (\"I want to order pizza\", \"sure, what toppings do you want\"),\n",
    "    (\"can I get a burger\", \"what size burger would you like\"),\n",
    "    (\"what is the weather\", \"the weather today is sunny\"),\n",
    "    (\"is it raining\", \"no rain expected today\"),\n",
    "    (\"hey, I want some pasta\", \"what kind of pasta would you prefer\"),\n",
    "    (\"do you have vegetarian options?\", \"yes, we have several vegetarian dishes\"),\n",
    "    (\"good morning\", \"good morning, how may I assist you?\"),\n",
    "    (\"bye\", \"take care, see you later\"),\n",
    "    (\"will it be hot today?\", \"expect warm temperatures all day\"),\n",
    "    (\"can I order a salad?\", \"what dressing would you like on your salad?\"),\n",
    "    (\"thanks, goodbye\", \"you're welcome, goodbye!\"),\n",
    "    (\"tell me the forecast\", \"the forecast shows clear skies\"),\n",
    "    (\"what's your name?\", \"i am your assistant, here to help\"),\n",
    "    (\"can I have a coffee?\", \"sure, would you like it black or with milk?\"),\n",
    "    (\"thank you for the help\", \"happy to assist you anytime\"),\n",
    "    (\"are you open today?\", \"yes, we are open from 9 am to 9 pm\"),\n",
    "    (\"could you help me with my order\", \"of course, what would you like to order\"),\n",
    "    (\"are there any gluten free options\", \"yes, we have several gluten free dishes available\"),\n",
    "    (\"what are today's specials\", \"today's special is grilled salmon with vegetables\"),\n",
    "    (\"how late are you open\", \"we are open until 10 pm tonight\"),\n",
    "    (\"can you recommend a dessert\", \"our chocolate lava cake is very popular\"),\n",
    "    (\"I need to change my order\", \"sure, what changes would you like to make\"),\n",
    "    (\"do you deliver\", \"yes, we deliver within a 5 mile radius\"),\n",
    "    (\"what payment methods do you accept\", \"we accept cash, credit cards, and mobile payments\"),\n",
    "    (\"is there a parking facility\", \"yes, free parking is available behind the restaurant\"),\n",
    "    (\"thank you very much\", \"you're welcome, happy to help\"),\n",
    "    (\"I have a food allergy\", \"please let us know your allergy, and we will accommodate\"),\n",
    "    (\"can I book a table\", \"yes, for how many people and what time\"),\n",
    "    (\"what's your restaurant address\", \"we are located at 123 Main Street\"),\n",
    "    (\"do you have vegan meals\", \"yes, we offer delicious vegan options\"),\n",
    "    (\"can I get nutritional information\", \"nutritional info is available on our website\"),\n",
    "    (\"how long is the wait time\", \"usually about 15 minutes during peak hours\"),\n",
    "    (\"do you have a kids menu\", \"yes, we have a special menu for children\"),\n",
    "    (\"can I cancel my order\", \"please provide your order number to cancel\"),\n",
    "    (\"what are your opening hours\", \"we are open from 9 am to 10 pm daily\"),\n",
    "    (\"is takeout available\", \"yes, you can order takeout anytime during opening hours\"),        \n",
    "]\n",
    "\n",
    "# Build vocabulary\n",
    "all_texts = [t[0] + \" \" + t[1] for t in train_data]\n",
    "all_words = set(word for sentence in all_texts for word in sentence.lower().split())\n",
    "word2idx = {w: i + 1 for i, w in enumerate(sorted(all_words))}\n",
    "idx2word = np.array(['<pad>'] + sorted(all_words))\n",
    "vocab_size = len(idx2word)\n",
    "\n",
    "max_input_len = 6\n",
    "max_resp_len = 8\n",
    "\n",
    "def encode_sentence(sent, max_len):\n",
    "    words = sent.lower().split()\n",
    "    seq = [word2idx.get(w, 0) for w in words]\n",
    "    seq = seq[:max_len] + [0] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "X_input = np.array([encode_sentence(t[0], max_input_len) for t in train_data])\n",
    "X_resp_in = np.array([encode_sentence(t[1], max_resp_len) for t in train_data])\n",
    "X_resp_out = np.array([encode_sentence(t[1], max_resp_len)[1:] + [0] for t in train_data]) # shifted\n",
    "\n",
    "class Expert(Model):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense2 = Dense(d_model)\n",
    "    def call(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "class GatingNetwork(Model):\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super().__init__()\n",
    "        self.dense = Dense(num_experts)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "    def call(self, x):\n",
    "        logits = self.dense(x)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "class MoEResponseGenerator(Model):\n",
    "    def __init__(self, vocab_size, d_model, num_experts, max_resp_len, lstm_units=128):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = [Expert(d_model) for _ in range(num_experts)]\n",
    "        self.gating_net = GatingNetwork(d_model, num_experts)\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        self.to_h = Dense(lstm_units)\n",
    "        self.to_c = Dense(lstm_units)\n",
    "        self.output_layer = Dense(vocab_size)\n",
    "        self.max_resp_len = max_resp_len\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_seq, resp_in_seq = inputs\n",
    "        enc_emb = self.embedding(input_seq)\n",
    "        pooled = self.global_pool(enc_emb)\n",
    "\n",
    "        gating_probs = self.gating_net(pooled)\n",
    "        expert_outs = tf.stack([expert(pooled) for expert in self.experts], axis=1)\n",
    "        gated_rep = tf.reduce_sum(tf.expand_dims(gating_probs, 2) * expert_outs, axis=1)\n",
    "\n",
    "        h_state = self.to_h(gated_rep)\n",
    "        c_state = self.to_c(gated_rep)\n",
    "\n",
    "        resp_emb = self.embedding(resp_in_seq)\n",
    "        lstm_out, _, _ = self.lstm(resp_emb, initial_state=[h_state, c_state])\n",
    "\n",
    "        logits = self.output_layer(lstm_out)\n",
    "        return logits, gating_probs\n",
    "\n",
    "# Hyperparams and dataset\n",
    "d_model = 32\n",
    "num_experts = 4\n",
    "batch_size = 2\n",
    "epochs = 250\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((X_input, X_resp_in), X_resp_out)).shuffle(20).batch(batch_size)\n",
    "\n",
    "model = MoEResponseGenerator(vocab_size, d_model, num_experts, max_resp_len)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, _ = model(inputs, training=True)\n",
    "        loss = loss_fn(labels, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for in_batch, out_batch in dataset:\n",
    "        loss = train_step(in_batch, out_batch)\n",
    "        total_loss += loss.numpy()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Loss: {total_loss/len(dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_logits(logits, temperature=1.0, top_k=5):\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        values, _ = tf.math.top_k(logits, k=top_k)\n",
    "        min_values = values[:, -1, None]\n",
    "        logits = tf.where(\n",
    "            logits < min_values,\n",
    "            tf.fill(tf.shape(logits), float('-inf')),\n",
    "            logits,\n",
    "        )\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    next_token = tf.random.categorical(tf.math.log(probabilities), num_samples=1)\n",
    "    return tf.squeeze(next_token, axis=-1).numpy()\n",
    "\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    input_seq = np.array([encode_sentence(input_text, max_input_len)])\n",
    "    response_seq = np.zeros((1, max_resp_len), dtype=np.int32)\n",
    "    generated_tokens = []\n",
    "    gating_probs = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        logits, gating_probs = model((input_seq, response_seq), training=False)\n",
    "        logits_step = logits[:, i % max_resp_len, :]\n",
    "        \n",
    "        next_token = sample_from_logits(logits_step, temperature=temperature, top_k=top_k)[0]\n",
    "\n",
    "        if next_token == 0:  # end on padding token\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(idx2word[next_token])\n",
    "        if i + 1 < max_resp_len:\n",
    "            response_seq[0, i] = next_token\n",
    "        else:\n",
    "            response_seq = np.roll(response_seq, -1, axis=1)\n",
    "            response_seq[0, -1] = next_token\n",
    "\n",
    "    top_expert = np.argmax(gating_probs[0].numpy()) if gating_probs is not None else -1\n",
    "    return \" \".join(generated_tokens), top_expert, gating_probs[0].numpy()\n",
    "\n",
    "\n",
    "# Demo\n",
    "inference_prompts = [\n",
    "    \"hello there\",\n",
    "    \"I want to order pizza\",\n",
    "    \"goodbye\",\n",
    "    \"can you help me order food\",\n",
    "    \"hi, what's going on?\",\n",
    "    \"will it rain tomorrow?\",\n",
    "    \"how do I say goodbye politely?\",\n",
    "    \"what toppings do you have?\",\n",
    "    \"is today sunny or cloudy?\",\n",
    "    \"see you soon\"\n",
    "]\n",
    "for input_text in inference_prompts:\n",
    "    response, expert_used, gating_distribution = generate_response(model, input_text, max_len=500)\n",
    "\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Generated response:\", response)\n",
    "    print(\"Top expert used:\", expert_used)\n",
    "    print(\"Gating probabilities:\", np.round(gating_distribution, 3))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_array = np.array(image)\n",
    "    mean_colors = image_array.mean(axis=(0, 1))\n",
    "    color_names = ['red', 'green', 'blue']\n",
    "    dominant_color = color_names[np.argmax(mean_colors)]\n",
    "    return f\"Dominant color is {dominant_color}.\"\n",
    "\n",
    "def multimodal_response(model, input_text=None, image_path=None):\n",
    "    result = \"\"\n",
    "    if input_text:\n",
    "        # Use your generate_response logic for text\n",
    "        response, exp_used, gating_distribution = generate_response(model, input_text, max_len=30)\n",
    "        result += f\"Text response: {response}\\nTop expert used: {exp_used}\\n\"\n",
    "    if image_path:\n",
    "        # Classify the image\n",
    "        image_result = classify_image(image_path)\n",
    "        result += f\"Image analysis: {image_result}\\n\"\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "print(multimodal_response(model, input_text=\"what toppings do you have?\", image_path=\"sample_image.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Softmax, LSTM, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# ----------- MNIST DIGIT CLASSIFIER -----------\n",
    "\n",
    "class DigitClassifier(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(32, (3,3), activation='relu')\n",
    "        self.pool1 = MaxPooling2D((2,2))\n",
    "        self.conv2 = Conv2D(64, (3,3), activation='relu')\n",
    "        self.pool2 = MaxPooling2D((2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(64, activation='relu')\n",
    "        self.out = Dense(10)  # digits 0-9 logits\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return self.out(x)\n",
    "\n",
    "def prepare_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# Train or load a pretrained digit classifier\n",
    "digit_classifier = DigitClassifier()\n",
    "(x_train, y_train), (x_test, y_test) = prepare_mnist_data()\n",
    "digit_classifier.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "print(\"Training digit classifier (this may take a bit)...\")\n",
    "digit_classifier.fit(x_train, y_train, epochs=3, batch_size=128, validation_split=0.1)\n",
    "\n",
    "# ----------- YOUR MOE TEXT GENERATION MODEL (copy your existing) -----------\n",
    "\n",
    "# (Paste your current MoEResponseGenerator, Expert, GatingNetwork, encode_sentence, sample_from_logits, etc. here)\n",
    "# For brevity, let's assume it's loaded as `moe_model`\n",
    "\n",
    "# For this example, just a placeholder generate_response:\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    # Use your real model's generation logic here\n",
    "    return f\"Simulated response to: {input_text}\", 1, np.array([0.1, 0.7, 0.1, 0.1])\n",
    "\n",
    "# ----------- MNIST INCREMENT LOGIC -----------\n",
    "\n",
    "def get_incremented_digit_image(input_image, x_dataset, y_dataset):\n",
    "    # Normalize pixel values and ensure float32\n",
    "    img = input_image.astype('float32')\n",
    "    if img.max() > 1.0:\n",
    "        img /= 255.0\n",
    "\n",
    "    # Add channel dimension if missing\n",
    "    if len(img.shape) == 2:   # grayscale image shape (28,28)\n",
    "        img = np.expand_dims(img, axis=-1)  # become (28,28,1)\n",
    "\n",
    "    # Add batch dimension for model input\n",
    "    img = np.expand_dims(img, axis=0)  # become (1,28,28,1)\n",
    "\n",
    "    logits = digit_classifier(img)\n",
    "    pred_digit = tf.argmax(logits, axis=1).numpy()[0]\n",
    "\n",
    "    inc_digit = 0 if pred_digit == 9 else pred_digit + 1\n",
    "    idx = np.where(y_dataset == inc_digit)[0][0]\n",
    "    inc_image = x_dataset[idx][:, :, 0]  # remove channel for display\n",
    "\n",
    "    return pred_digit, inc_digit, inc_image\n",
    "\n",
    "\n",
    "# ----------- MULTIMODAL HANDLER -----------\n",
    "\n",
    "def multimodal_handler(input_text=None, input_image=None):\n",
    "    if input_text:\n",
    "        response, expert, gating = generate_response(None, input_text)\n",
    "        print(f\"Text Input: {input_text}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Top expert used: {expert}\")\n",
    "        print(f\"Gating probabilities: {gating}\")\n",
    "    if input_image is not None:\n",
    "        pred_digit, inc_digit, inc_image = get_incremented_digit_image(input_image, x_test, np.argmax(y_test, axis=1))\n",
    "        print(f\"Input Digit: {pred_digit}, Incremented Digit: {inc_digit}\")\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title(f\"Input: {pred_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(inc_image, cmap='gray')\n",
    "        plt.title(f\"Output: {inc_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# ----------- DEMO -----------\n",
    "\n",
    "# Text-only\n",
    "multimodal_handler(input_text=\"I need help with my order.\")\n",
    "\n",
    "# Image-only\n",
    "multimodal_handler(input_image=x_test[5])\n",
    "\n",
    "# Both modalities\n",
    "multimodal_handler(input_text=\"What digit is this?\", input_image=x_test[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
