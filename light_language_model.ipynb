{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n",
      "Epoch 1\n",
      "Train Accuracy: 0.2500\n",
      "Epoch 2\n",
      "Train Accuracy: 0.2500\n",
      "Epoch 3\n",
      "Train Accuracy: 0.5000\n",
      "Epoch 4\n",
      "Train Accuracy: 0.5000\n",
      "Epoch 5\n",
      "Train Accuracy: 0.5000\n",
      "Epoch 6\n",
      "Train Accuracy: 0.5000\n",
      "Epoch 7\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 8\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 9\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 10\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 11\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 12\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 13\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 14\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 15\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 16\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 17\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 18\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 19\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 20\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 21\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 22\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 23\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 24\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 25\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 26\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 27\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 28\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 29\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 30\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 31\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 32\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 33\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 34\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 35\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 36\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 37\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 38\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 39\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 40\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 41\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 42\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 43\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 44\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 45\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 46\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 47\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 48\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 49\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 50\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 51\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 52\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 53\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 54\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 55\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 56\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 57\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 58\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 59\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 60\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 61\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 62\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 63\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 64\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 65\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 66\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 67\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 68\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 69\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 70\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 71\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 72\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 73\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 74\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 75\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 76\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 77\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 78\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 79\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 80\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 81\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 82\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 83\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 84\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 85\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 86\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 87\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 88\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 89\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 90\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 91\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 92\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 93\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 94\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 95\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 96\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 97\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 98\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 99\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 100\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 101\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 102\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 103\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 104\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 105\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 106\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 107\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 108\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 109\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 110\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 111\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 112\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 113\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 114\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 115\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 116\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 117\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 118\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 119\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 120\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 121\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 122\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 123\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 124\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 125\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 126\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 127\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 128\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 129\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 130\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 131\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 132\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 133\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 134\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 135\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 136\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 137\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 138\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 139\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 140\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 141\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 142\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 143\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 144\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 145\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 146\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 147\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 148\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 149\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 150\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 151\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 152\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 153\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 154\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 155\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 156\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 157\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 158\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 159\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 160\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 161\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 162\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 163\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 164\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 165\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 166\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 167\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 168\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 169\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 170\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 171\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 172\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 173\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 174\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 175\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 176\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 177\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 178\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 179\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 180\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 181\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 182\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 183\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 184\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 185\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 186\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 187\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 188\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 189\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 190\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 191\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 192\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 193\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 194\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 195\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 196\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 197\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 198\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 199\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 200\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 201\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 202\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 203\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 204\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 205\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 206\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 207\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 208\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 209\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 210\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 211\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 212\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 213\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 214\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 215\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 216\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 217\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 218\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 219\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 220\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 221\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 222\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 223\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 224\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 225\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 226\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 227\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 228\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 229\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 230\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 231\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 232\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 233\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 234\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 235\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 236\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 237\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 238\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 239\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 240\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 241\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 242\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 243\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 244\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 245\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 246\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 247\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 248\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 249\n",
      "Train Accuracy: 0.7500\n",
      "Epoch 250\n",
      "Train Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Vocabulary definitions\n",
    "sound_vocab = {\"<pad>\": 0, \"cat\": 1, \"dog\": 2, \"meow\": 3, \"bark\": 4}\n",
    "inv_sound_vocab = {v: k for k, v in sound_vocab.items()}\n",
    "animal_vocab = {\"<pad>\": 0, \"cat\": 1, \"dog\": 2}\n",
    "inv_animal_vocab = {v: k for k, v in animal_vocab.items()}\n",
    "\n",
    "dummy_text = \"<empty>\"\n",
    "dummy_image = np.zeros((224,224,3), dtype=np.float32)\n",
    "\n",
    "# Text expert with Universal Sentence Encoder\n",
    "class HubEmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hub_url=\"https://tfhub.dev/google/universal-sentence-encoder/4\"):\n",
    "        super().__init__()\n",
    "        self.hub_layer = hub.KerasLayer(hub_url, trainable=True)\n",
    "    def call(self, inputs):\n",
    "        return self.hub_layer(inputs)\n",
    "\n",
    "def build_text_expert():\n",
    "    inp = Input(shape=(), dtype=tf.string)\n",
    "    x = HubEmbeddingLayer()(inp)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(len(sound_vocab))(x)\n",
    "    return Model(inp, out, name=\"TextExpert\")\n",
    "\n",
    "# Image expert using MobileNetV2\n",
    "def build_image_expert():\n",
    "    inp = Input(shape=(224,224,3))\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(224,224,3), include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "    )\n",
    "    base_model.trainable = True\n",
    "    x = base_model(inp)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    out = layers.Dense(len(animal_vocab))(x)\n",
    "    return Model(inp, out, name=\"ImageExpert\")\n",
    "\n",
    "# Data augmentation pipeline for both cats and dogs\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.3),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomContrast(0.3),\n",
    "    layers.RandomBrightness(0.2),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "])\n",
    "\n",
    "# Gating network\n",
    "def build_gate():\n",
    "    text_in = Input(shape=(), dtype=tf.string)\n",
    "    image_in = Input(shape=(224,224,3))\n",
    "    text_present = Input(shape=(1,), dtype=tf.float32)\n",
    "    image_present = Input(shape=(1,), dtype=tf.float32)\n",
    "\n",
    "    text_feat = HubEmbeddingLayer()(text_in)\n",
    "    text_feat = layers.Dense(32, activation=\"relu\")(text_feat)\n",
    "\n",
    "    x = layers.Conv2D(16, (3,3), strides=(2,2), activation=\"relu\")(image_in)\n",
    "    x = layers.Flatten()(x)\n",
    "    image_feat = layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "    merged = layers.Concatenate()([text_feat, image_feat, text_present, image_present])\n",
    "    x = layers.Dense(32, activation=\"relu\")(merged)\n",
    "    logits = layers.Dense(2)(x)\n",
    "    gate_weights = layers.Softmax()(logits)\n",
    "    return Model([text_in, image_in, text_present, image_present], gate_weights, name=\"Gate\")\n",
    "\n",
    "class SoftmaxWithTemp(Layer):\n",
    "    def __init__(self, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.softmax(inputs / self.temperature)\n",
    "\n",
    "class PadLayer(Layer):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "    def call(self, inputs):\n",
    "        neg_inf = tf.fill([tf.shape(inputs)[0], self.pad_width], -1e9)\n",
    "        return tf.concat([inputs, neg_inf], axis=1)\n",
    "\n",
    "def gating_entropy_loss(gate_weights):\n",
    "    return -tf.reduce_mean(tf.reduce_sum(gate_weights * tf.math.log(gate_weights + 1e-10), axis=1))\n",
    "\n",
    "def build_moe_model():\n",
    "    text_input = Input(shape=(), dtype=tf.string)\n",
    "    image_input = Input(shape=(224,224,3))\n",
    "    text_present = Input(shape=(1,), dtype=tf.float32)\n",
    "    image_present = Input(shape=(1,), dtype=tf.float32)\n",
    "\n",
    "    text_expert = build_text_expert()\n",
    "    image_expert = build_image_expert()\n",
    "    gate = build_gate()\n",
    "\n",
    "    gate_weights = gate([text_input, image_input, text_present, image_present])\n",
    "    text_logits = text_expert(text_input)\n",
    "    image_logits = image_expert(image_input)\n",
    "\n",
    "    pad_width = len(sound_vocab) - len(animal_vocab)  # 5-3=2\n",
    "    pad_layer = PadLayer(pad_width)\n",
    "\n",
    "    image_logits_padded = pad_layer(image_logits)\n",
    "\n",
    "    softmax_layer = SoftmaxWithTemp(temperature=0.7)\n",
    "    text_logits_norm = softmax_layer(text_logits)\n",
    "    image_logits_norm = softmax_layer(image_logits_padded)\n",
    "\n",
    "    combined_logits = gate_weights[:,0:1]*text_logits_norm + gate_weights[:,1:2]*image_logits_norm\n",
    "\n",
    "    out = layers.Activation(\"softmax\")(combined_logits)\n",
    "\n",
    "    return Model([text_input, image_input, text_present, image_present], out), text_expert, image_expert, gate\n",
    "\n",
    "moe_model, text_expert, image_expert, gate_model = build_moe_model()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "def create_dummy_images(num, val):\n",
    "    return np.full((num, 224, 224, 3), val, dtype=np.float32)\n",
    "\n",
    "text_inputs = [\n",
    "    \"cat\", \"dog\", dummy_text, dummy_text, \"cat\", \"dog\", \"cat\", \"dog\",\n",
    "]\n",
    "\n",
    "image_inputs = np.concatenate([\n",
    "    create_dummy_images(2, 0.0),\n",
    "    create_dummy_images(2, 0.5),\n",
    "    create_dummy_images(2, 1.0),\n",
    "    create_dummy_images(1, 1.0),\n",
    "    create_dummy_images(1, 0.5),\n",
    "])\n",
    "\n",
    "labels = np.array([\n",
    "    sound_vocab[\"meow\"], sound_vocab[\"bark\"],\n",
    "    animal_vocab[\"cat\"], animal_vocab[\"dog\"],\n",
    "    animal_vocab[\"cat\"], animal_vocab[\"dog\"],\n",
    "    animal_vocab[\"dog\"], animal_vocab[\"cat\"],\n",
    "])\n",
    "\n",
    "text_present = np.array([1,1,0,0,1,1,1,1], dtype=np.float32)\n",
    "image_present = np.array([0,0,1,1,1,1,1,1], dtype=np.float32)\n",
    "\n",
    "text_tensor = tf.constant(text_inputs)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((text_tensor, image_inputs, text_present, image_present), labels)\n",
    ").batch(2).shuffle(buffer_size=8)\n",
    "\n",
    "lambda_entropy = 0.01\n",
    "lambda_image_aux = 0.8\n",
    "\n",
    "@tf.function\n",
    "def train_step(t_b, i_b, tp_b, ip_b, lbl_b):\n",
    "    with tf.GradientTape() as tape:\n",
    "        gate_weights = gate_model([t_b, i_b, tp_b, ip_b], training=True)\n",
    "        preds = moe_model([t_b, i_b, tp_b, ip_b], training=True)\n",
    "        text_logits = text_expert(t_b, training=True)\n",
    "        image_logits = image_expert(i_b, training=True)\n",
    "\n",
    "        main_loss = loss_fn(lbl_b, preds)\n",
    "\n",
    "        # Mask for image present examples\n",
    "        mask = tf.squeeze(ip_b) == 1.0\n",
    "        image_labels = tf.boolean_mask(lbl_b, mask)\n",
    "        image_logits_masked = tf.boolean_mask(image_logits, mask)\n",
    "        aux_loss = loss_fn(image_labels, image_logits_masked) if tf.size(image_labels) > 0 else 0.0\n",
    "\n",
    "        entropy_loss = gating_entropy_loss(gate_weights)\n",
    "\n",
    "        total_loss = main_loss + 0.8*aux_loss + 0.01*entropy_loss\n",
    "\n",
    "    grads = tape.gradient(total_loss, moe_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, moe_model.trainable_weights))\n",
    "    train_acc_metric.update_state(lbl_b, preds)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "print(\"Training Started\")\n",
    "for epoch in range(250):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    train_acc_metric.reset_state()\n",
    "    for (t_b, i_b, tp_b, ip_b), lbl_b in train_dataset:\n",
    "        loss = train_step(t_b, i_b, tp_b, ip_b, lbl_b)\n",
    "    print(f\"Train Accuracy: {train_acc_metric.result():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: text input 'dog'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627ms/step\n",
      "Softmax output vector: [0.14885008 0.14885756 0.1488506  0.14885165 0.40459013]\n",
      "Input: text='dog', image_present=0.0\n",
      "Prediction: bark\n",
      "--------------------------------------------------\n",
      "Test: text input 'canine'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Softmax output vector: [0.14890158 0.14900288 0.14891769 0.14897619 0.40420166]\n",
      "Input: text='canine', image_present=0.0\n",
      "Prediction: bark\n",
      "--------------------------------------------------\n",
      "Test: text input 'cat'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Softmax output vector: [0.14884998 0.1488574  0.14884986 0.40459168 0.14885104]\n",
      "Input: text='cat', image_present=0.0\n",
      "Prediction: meow\n",
      "--------------------------------------------------\n",
      "Test: text input 'feline'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Softmax output vector: [0.14892425 0.14905533 0.14891747 0.4040837  0.1490192 ]\n",
      "Input: text='feline', image_present=0.0\n",
      "Prediction: meow\n",
      "--------------------------------------------------\n",
      "Test: image of cat\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Softmax output vector: [0.14887406 0.40450403 0.14889997 0.14886092 0.14886092]\n",
      "Input: text='<empty>', image_present=1.0\n",
      "Prediction: cat\n",
      "--------------------------------------------------\n",
      "Test: image of dog\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Softmax output vector: [0.1488476 0.1488476 0.4046097 0.1488476 0.1488476]\n",
      "Input: text='<empty>', image_present=1.0\n",
      "Prediction: dog\n",
      "--------------------------------------------------\n",
      "Test: text 'cat' and image cat\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Softmax output vector: [0.14887406 0.40450403 0.14889997 0.14886092 0.14886092]\n",
      "Input: text='cat', image_present=1.0\n",
      "Prediction: cat\n",
      "--------------------------------------------------\n",
      "Test: text 'cat' and image dog\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Softmax output vector: [0.1488476 0.1488476 0.4046097 0.1488476 0.1488476]\n",
      "Input: text='cat', image_present=1.0\n",
      "Prediction: dog\n",
      "--------------------------------------------------\n",
      "Test: text 'dog' and image dog\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Softmax output vector: [0.1488476 0.1488476 0.4046097 0.1488476 0.1488476]\n",
      "Input: text='dog', image_present=1.0\n",
      "Prediction: dog\n",
      "--------------------------------------------------\n",
      "Test: text 'dog' and image cat\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Softmax output vector: [0.14887406 0.40450403 0.14889997 0.14886092 0.14886092]\n",
      "Input: text='dog', image_present=1.0\n",
      "Prediction: cat\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction function\n",
    "def predict_with_moe(text_input=None, image_input=None):\n",
    "    if text_input is None or text_input == \"\":\n",
    "        text_input = dummy_text\n",
    "    if image_input is None:\n",
    "        image_input = dummy_image\n",
    "\n",
    "    if image_input.ndim == 3 and image_input.shape[2] == 1:\n",
    "        image_input = np.concatenate([image_input]*3, axis=2)\n",
    "\n",
    "    if image_input.shape != (224,224,3):\n",
    "        image_input = tf.image.resize(image_input, (224,224)).numpy()\n",
    "\n",
    "    text_present_f = 0.0 if text_input == dummy_text else 1.0\n",
    "    image_present_f = 0.0 if np.all(image_input == 0) else 1.0\n",
    "\n",
    "    preds = moe_model.predict([\n",
    "        tf.constant([text_input]),\n",
    "        np.array([image_input]),\n",
    "        np.array([[text_present_f]], dtype=np.float32),\n",
    "        np.array([[image_present_f]], dtype=np.float32)\n",
    "    ])\n",
    "\n",
    "    if image_present_f == 1.0:  # Use image vocab slice (excluding padded)\n",
    "        image_logits = preds[0][1:1+len(animal_vocab)-1]  # indices 1 to 3 (cat,dog)\n",
    "        pred_id = np.argmax(image_logits) + 1  # offset because padding at 0\n",
    "        pred_label = inv_animal_vocab.get(pred_id, \"unknown\")\n",
    "    else:\n",
    "        pred_id = np.argmax(preds[0])\n",
    "        pred_label = inv_sound_vocab.get(pred_id, \"unknown\")\n",
    "\n",
    "    print(f\"Softmax output vector: {preds[0]}\")\n",
    "    pred_id = np.argmax(preds[0])\n",
    "    if image_present_f == 1.0:\n",
    "        pred_label = inv_animal_vocab.get(pred_id, \"unknown\")\n",
    "    else:\n",
    "        pred_label = inv_sound_vocab.get(pred_id, \"unknown\")\n",
    "    print(f\"Input: text='{text_input}', image_present={image_present_f}\")\n",
    "    print(f\"Prediction: {pred_label}\\n{'-'*50}\")\n",
    "    return pred_label\n",
    "\n",
    "# Example tests\n",
    "print(\"Test: text input 'dog'\")\n",
    "predict_with_moe(text_input=\"dog\")\n",
    "print(\"Test: text input 'canine'\")\n",
    "predict_with_moe(text_input=\"canine\")\n",
    "print(\"Test: text input 'cat'\")\n",
    "predict_with_moe(text_input=\"cat\")\n",
    "print(\"Test: text input 'feline'\")\n",
    "predict_with_moe(text_input=\"feline\")\n",
    "print(\"Test: image of cat\")\n",
    "predict_with_moe(image_input=np.ones((4,4,1))*0.5)\n",
    "print(\"Test: image of dog\")\n",
    "predict_with_moe(image_input=np.ones((4,4,1)))\n",
    "print(\"Test: text 'cat' and image cat\")\n",
    "predict_with_moe(text_input=\"cat\", image_input=np.ones((4,4,1))*0.5)\n",
    "print(\"Test: text 'cat' and image dog\")\n",
    "predict_with_moe(text_input=\"cat\", image_input=np.ones((4,4,1)))\n",
    "print(\"Test: text 'dog' and image dog\")\n",
    "predict_with_moe(text_input=\"dog\", image_input=np.ones((4,4,1)))\n",
    "print(\"Test: text 'dog' and image cat\")\n",
    "predict_with_moe(text_input=\"dog\", image_input=np.ones((4,4,1))*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Softmax, LSTM, Conv2D, Flatten, MaxPooling2D, LayerNormalization, Dropout, Input, Conv2DTranspose, Reshape, Add, Lambda, BatchNormalization, add, Activation\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# MoE Text Generation Model Components\n",
    "# ---------------------------------------------\n",
    "\n",
    "class Expert(Model):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.dense2 = Dense(d_model)\n",
    "    def call(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "class GatingNetwork(Model):\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super().__init__()\n",
    "        self.dense = Dense(num_experts)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "    def call(self, x):\n",
    "        logits = self.dense(x)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "class MoEResponseGenerator(Model):\n",
    "    def __init__(self, vocab_size, d_model, num_experts, max_resp_len, lstm_units=128):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = [Expert(d_model) for _ in range(num_experts)]\n",
    "        self.gating_net = GatingNetwork(d_model, num_experts)\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        self.to_h = Dense(lstm_units)\n",
    "        self.to_c = Dense(lstm_units)\n",
    "        self.output_layer = Dense(vocab_size)\n",
    "        self.max_resp_len = max_resp_len\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_seq, resp_in_seq = inputs\n",
    "        enc_emb = self.embedding(input_seq)\n",
    "        pooled = self.global_pool(enc_emb)\n",
    "\n",
    "        gating_probs = self.gating_net(pooled)\n",
    "        expert_outs = tf.stack([expert(pooled) for expert in self.experts], axis=1)\n",
    "        gated_rep = tf.reduce_sum(tf.expand_dims(gating_probs, 2) * expert_outs, axis=1)\n",
    "\n",
    "        h_state = self.to_h(gated_rep)\n",
    "        c_state = self.to_c(gated_rep)\n",
    "\n",
    "        resp_emb = self.embedding(resp_in_seq)\n",
    "        lstm_out, _, _ = self.lstm(resp_emb, initial_state=[h_state, c_state])\n",
    "\n",
    "        logits = self.output_layer(lstm_out)\n",
    "        return logits, gating_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Utility functions for text encoding and generation\n",
    "# ---------------------------------------------\n",
    "\n",
    "# (Your train_data list is assumed defined here - omitted for brevity, but same as your list)\n",
    "\n",
    "# Build vocabulary based on train_data (input + responses)\n",
    "train_data = [\n",
    "    (\"hello there\", \"hi, how can I help\"),\n",
    "    (\"hi\", \"hello, what can I do\"),\n",
    "    (\"goodbye\", \"goodbye, have a nice day\"),\n",
    "    (\"see you later\", \"see you soon, goodbye\"),\n",
    "    (\"I want to order pizza\", \"sure, what toppings do you want\"),\n",
    "    (\"can I get a burger\", \"what size burger would you like\"),\n",
    "    (\"what is the weather\", \"the weather today is sunny\"),\n",
    "    (\"is it raining\", \"no rain expected today\"),\n",
    "    (\"hey, I want some pasta\", \"what kind of pasta would you prefer\"),\n",
    "    (\"do you have vegetarian options?\", \"yes, we have several vegetarian dishes\"),\n",
    "    (\"good morning\", \"good morning, how may I assist you?\"),\n",
    "    (\"bye\", \"take care, see you later\"),\n",
    "    (\"will it be hot today?\", \"expect warm temperatures all day\"),\n",
    "    (\"can I order a salad?\", \"what dressing would you like on your salad?\"),\n",
    "    (\"thanks, goodbye\", \"you're welcome, goodbye!\"),\n",
    "    (\"tell me the forecast\", \"the forecast shows clear skies\"),\n",
    "    (\"what's your name?\", \"i am your assistant, here to help\"),\n",
    "    (\"can I have a coffee?\", \"sure, would you like it black or with milk?\"),\n",
    "    (\"thank you for the help\", \"happy to assist you anytime\"),\n",
    "    (\"are you open today?\", \"yes, we are open from 9 am to 9 pm\"),\n",
    "    (\"could you help me with my order\", \"of course, what would you like to order\"),\n",
    "    (\"are there any gluten free options\", \"yes, we have several gluten free dishes available\"),\n",
    "    (\"what are today's specials\", \"today's special is grilled salmon with vegetables\"),\n",
    "    (\"how late are you open\", \"we are open until 10 pm tonight\"),\n",
    "    (\"can you recommend a dessert\", \"our chocolate lava cake is very popular\"),\n",
    "    (\"I need to change my order\", \"sure, what changes would you like to make\"),\n",
    "    (\"do you deliver\", \"yes, we deliver within a 5 mile radius\"),\n",
    "    (\"what payment methods do you accept\", \"we accept cash, credit cards, and mobile payments\"),\n",
    "    (\"is there a parking facility\", \"yes, free parking is available behind the restaurant\"),\n",
    "    (\"thank you very much\", \"you're welcome, happy to help\"),\n",
    "    (\"I have a food allergy\", \"please let us know your allergy, and we will accommodate\"),\n",
    "    (\"can I book a table\", \"yes, for how many people and what time\"),\n",
    "    (\"what's your restaurant address\", \"we are located at 123 Main Street\"),\n",
    "    (\"do you have vegan meals\", \"yes, we offer delicious vegan options\"),\n",
    "    (\"can I get nutritional information\", \"nutritional info is available on our website\"),\n",
    "    (\"how long is the wait time\", \"usually about 15 minutes during peak hours\"),\n",
    "    (\"do you have a kids menu\", \"yes, we have a special menu for children\"),\n",
    "    (\"can I cancel my order\", \"please provide your order number to cancel\"),\n",
    "    (\"what are your opening hours\", \"we are open from 9 am to 10 pm daily\"),\n",
    "    (\"is takeout available\", \"yes, you can order takeout anytime during opening hours\"),        \n",
    "]\n",
    "\n",
    "all_texts = [t[0] + \" \" + t[1] for t in train_data]\n",
    "all_words = set(word for sentence in all_texts for word in sentence.lower().split())\n",
    "word2idx = {w: i + 1 for i, w in enumerate(sorted(all_words))}\n",
    "idx2word = np.array(['<pad>'] + sorted(all_words))\n",
    "vocab_size = len(idx2word)\n",
    "max_input_len = 6\n",
    "max_resp_len = 8\n",
    "\n",
    "def encode_sentence(sent, max_len):\n",
    "    words = sent.lower().split()\n",
    "    seq = [word2idx.get(w, 0) for w in words]\n",
    "    seq = seq[:max_len] + [0] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0, top_k=5):\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        values, _ = tf.math.top_k(logits, k=top_k)\n",
    "        min_values = values[:, -1, None]\n",
    "        logits = tf.where(\n",
    "            logits < min_values,\n",
    "            tf.fill(tf.shape(logits), float('-inf')),\n",
    "            logits,\n",
    "        )\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    next_token = tf.random.categorical(tf.math.log(probabilities), num_samples=1)\n",
    "    return tf.squeeze(next_token, axis=-1).numpy()\n",
    "\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    input_seq = np.array([encode_sentence(input_text, max_input_len)])\n",
    "    response_seq = np.zeros((1, max_resp_len), dtype=np.int32)\n",
    "    generated_tokens = []\n",
    "    gating_probs = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        logits, gating_probs = model((input_seq, response_seq), training=False)\n",
    "        logits_step = logits[:, i % max_resp_len, :]\n",
    "        next_token = sample_from_logits(logits_step, temperature=temperature, top_k=top_k)[0]\n",
    "        if next_token == 0:\n",
    "            break\n",
    "        generated_tokens.append(idx2word[next_token])\n",
    "        if i + 1 < max_resp_len:\n",
    "            response_seq[0, i] = next_token\n",
    "        else:\n",
    "            response_seq = np.roll(response_seq, -1, axis=1)\n",
    "            response_seq[0, -1] = next_token\n",
    "\n",
    "    top_expert = np.argmax(gating_probs[0].numpy()) if gating_probs is not None else -1\n",
    "    return \" \".join(generated_tokens), top_expert, gating_probs[0].numpy()\n",
    "\n",
    "# Build training dataset\n",
    "X_input = np.array([encode_sentence(t[0], max_input_len) for t in train_data])\n",
    "X_resp_in = np.array([encode_sentence(t[1], max_resp_len) for t in train_data])\n",
    "X_resp_out = np.array([encode_sentence(t[1], max_resp_len)[1:] + [0] for t in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# MNIST Digit Classifier Components\n",
    "# ---------------------------------------------\n",
    "\n",
    "class DigitClassifier(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(32, (3,3), activation='relu')\n",
    "        self.pool1 = MaxPooling2D((2,2))\n",
    "        self.conv2 = Conv2D(64, (3,3), activation='relu')\n",
    "        self.pool2 = MaxPooling2D((2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(64, activation='relu')\n",
    "        self.out = Dense(10)  # digits 0-9 logits\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return self.out(x)\n",
    "\n",
    "def prepare_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Training setup and training loops\n",
    "# ---------------------------------------------\n",
    "\n",
    "digit_classifier = DigitClassifier()\n",
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = prepare_mnist_data()\n",
    "digit_classifier.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "print(\"Training MNIST digit classifier...\")\n",
    "digit_classifier.fit(x_train_mnist, y_train_mnist, epochs=3, batch_size=128, validation_split=0.1)\n",
    "\n",
    "d_model = 32\n",
    "num_experts = 5\n",
    "lstm_units = 128\n",
    "max_epochs = 300\n",
    "batch_size = 2\n",
    "\n",
    "moe_model = MoEResponseGenerator(vocab_size, d_model, num_experts, max_resp_len, lstm_units)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((X_input, X_resp_in), X_resp_out)).shuffle(50).batch(batch_size)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, _ = moe_model(inputs, training=True)\n",
    "        loss = loss_fn(labels, logits)\n",
    "    grads = tape.gradient(loss, moe_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, moe_model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "print(\"Training MoE text generation model...\")\n",
    "for epoch in range(max_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in train_dataset:\n",
    "        loss = train_step(batch[0], batch[1])\n",
    "        total_loss += loss.numpy()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{max_epochs}: Loss = {total_loss / len(train_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "# Load and normalize MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Prepare target images for increment digits\n",
    "train_target_images = np.zeros_like(x_train)\n",
    "for idx in range(len(x_train)):\n",
    "    inc_digit = (y_train[idx] + 1) % 10\n",
    "    candidates = np.where(y_train == inc_digit)[0]\n",
    "    chosen_idx = np.random.choice(candidates)\n",
    "    train_target_images[idx] = x_train[chosen_idx]\n",
    "\n",
    "test_target_images = np.zeros_like(x_test)\n",
    "for idx in range(len(x_test)):\n",
    "    inc_digit = (y_test[idx] + 1) % 10\n",
    "    candidates = np.where(y_test == inc_digit)[0]\n",
    "    chosen_idx = np.random.choice(candidates)\n",
    "    test_target_images[idx] = x_test[chosen_idx]\n",
    "\n",
    "# Residual block\n",
    "def residual_block(x, filters, kernel_size=3):\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = add([shortcut, x])\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "# Build model with residual blocks\n",
    "def build_improved_increment_model():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    x = Conv2D(64, 3, strides=2, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x = Conv2D(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = residual_block(x, 128)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(7*7*128, activation=\"relu\")(x)\n",
    "    x = Reshape((7,7,128))(x)\n",
    "    x = Conv2DTranspose(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2DTranspose(64, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Conv2D(1, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "increment_model = build_improved_increment_model()\n",
    "\n",
    "# Initialize VGG16 model for perceptual loss - with input shape 32x32x3 (minimum required)\n",
    "vgg = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "vgg.trainable = False\n",
    "feature_extractor = Model(vgg.input, vgg.get_layer(\"block3_conv3\").output)\n",
    "\n",
    "# Preprocessing function to resize and replicate grayscale channel for VGG16\n",
    "def preprocess_for_vgg(x):\n",
    "    x_resized = tf.image.resize(x, [32, 32])\n",
    "    x_rgb = tf.image.grayscale_to_rgb(x_resized)\n",
    "    x_preprocessed = preprocess_input(x_rgb * 255.0)\n",
    "    return x_preprocessed\n",
    "\n",
    "# Perceptual loss combining MSE and VGG feature loss\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    y_true_pp = preprocess_for_vgg(y_true)\n",
    "    y_pred_pp = preprocess_for_vgg(y_pred)\n",
    "    f_true = feature_extractor(y_true_pp)\n",
    "    f_pred = feature_extractor(y_pred_pp)\n",
    "    return tf.reduce_mean(tf.square(f_true - f_pred))\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    pl = perceptual_loss(y_true, y_pred)\n",
    "    return mse + 0.1 * pl\n",
    "\n",
    "increment_model.compile(optimizer='adam', loss=combined_loss)\n",
    "\n",
    "# Train with learning rate scheduler callback\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "increment_model.fit(\n",
    "    x_train,\n",
    "    train_target_images,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_test, test_target_images),\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n",
    "\n",
    "# Inference helper\n",
    "def get_incremented_digit_image(input_image):\n",
    "    img = input_image.astype(\"float32\") / 255.0\n",
    "    if img.ndim == 2:\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    generated_img = increment_model.predict(img)\n",
    "    return generated_img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of test results for a random sample\n",
    "# Visualization test\n",
    "idx = np.random.randint(len(x_test))\n",
    "input_img = x_test[idx]\n",
    "original_digit = y_test[idx]\n",
    "inc_digit = (original_digit + 1) % 10\n",
    "generated_img = get_incremented_digit_image(input_img)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(f\"Input: {original_digit}\")\n",
    "plt.imshow(input_img[:,:,0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(f\"Generated: {inc_digit}\")\n",
    "plt.imshow(generated_img[:,:,0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(f\"Expected: {inc_digit}\")\n",
    "expected_img = x_test[np.where(y_test == inc_digit)[0][0]]\n",
    "plt.imshow(expected_img[:,:,0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_handler(input_text=None, input_image=None):\n",
    "    if input_text:\n",
    "        response, expert, gating_probs = generate_response(moe_model, input_text)\n",
    "        print(f\"Text Input: {input_text}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Top expert used: {expert}\")\n",
    "        print(f\"Gating probs: {np.round(gating_probs, 3)}\")\n",
    "    if input_image is not None:\n",
    "        pred_digit, inc_digit, inc_image = get_incremented_digit_image_generative(input_image)\n",
    "        print(f\"Image input digit: {pred_digit}, incremented output digit: {inc_digit}\")\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title(f\"Input: {pred_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(inc_image, cmap='gray')\n",
    "        plt.title(f\"Output: {inc_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Demo code to test multimodal system\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Text only\n",
    "multimodal_handler(input_text=\"hello, how are you?\")\n",
    "\n",
    "# Image only (from MNIST test)\n",
    "multimodal_handler(input_image=x_test_mnist[15])\n",
    "\n",
    "# Both\n",
    "multimodal_handler(input_text=\"What number is this?\", input_image=x_test_mnist[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_examples = [\n",
    "    \"hello\",\n",
    "    \"hi there\",\n",
    "    \"good morning\",\n",
    "    \"hey, how are you?\",\n",
    "    \"what's up?\"\n",
    "]\n",
    "\n",
    "goodbye_examples = [\n",
    "    \"goodbye\",\n",
    "    \"see you later\",\n",
    "    \"bye for now\",\n",
    "    \"talk to you soon\",\n",
    "    \"have a great day\"\n",
    "]\n",
    "\n",
    "order_food_examples = [\n",
    "    \"I want to order a pizza\",\n",
    "    \"can I get a burger please?\",\n",
    "    \"what sides do you have?\",\n",
    "    \"I'd like a vegetarian pasta\",\n",
    "    \"do you have gluten free options?\"\n",
    "]\n",
    "\n",
    "weather_examples = [\n",
    "    \"what's the weather today?\",\n",
    "    \"will it rain tomorrow?\",\n",
    "    \"is it sunny outside?\",\n",
    "    \"what's the forecast for this week?\",\n",
    "    \"do I need an umbrella today?\"\n",
    "]\n",
    "\n",
    "miscellaneous_examples = [\n",
    "    \"what's your name?\",\n",
    "    \"can you tell me a joke?\",\n",
    "    \"how do I say goodbye politely?\",\n",
    "    \"are you open on weekends?\",\n",
    "    \"what time do you close?\"\n",
    "]\n",
    "\n",
    "inference_prompts = greeting_examples + goodbye_examples + order_food_examples + weather_examples + miscellaneous_examples\n",
    "for input_text in inference_prompts:\n",
    "    multimodal_handler(input_text)\n",
    "\n",
    "    # print(\"Input:\", input_text)\n",
    "    # print(\"Generated response:\", response)\n",
    "    # print(\"Top expert used:\", expert_used)\n",
    "    # print(\"Gating probabilities:\", np.round(gating_distribution, 3))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (input, response)\n",
    "train_data = [\n",
    "    (\"hello there\", \"hi, how can I help\"),\n",
    "    (\"hi\", \"hello, what can I do\"),\n",
    "    (\"goodbye\", \"goodbye, have a nice day\"),\n",
    "    (\"see you later\", \"see you soon, goodbye\"),\n",
    "    (\"I want to order pizza\", \"sure, what toppings do you want\"),\n",
    "    (\"can I get a burger\", \"what size burger would you like\"),\n",
    "    (\"what is the weather\", \"the weather today is sunny\"),\n",
    "    (\"is it raining\", \"no rain expected today\"),\n",
    "    (\"hey, I want some pasta\", \"what kind of pasta would you prefer\"),\n",
    "    (\"do you have vegetarian options?\", \"yes, we have several vegetarian dishes\"),\n",
    "    (\"good morning\", \"good morning, how may I assist you?\"),\n",
    "    (\"bye\", \"take care, see you later\"),\n",
    "    (\"will it be hot today?\", \"expect warm temperatures all day\"),\n",
    "    (\"can I order a salad?\", \"what dressing would you like on your salad?\"),\n",
    "    (\"thanks, goodbye\", \"you're welcome, goodbye!\"),\n",
    "    (\"tell me the forecast\", \"the forecast shows clear skies\"),\n",
    "    (\"what's your name?\", \"i am your assistant, here to help\"),\n",
    "    (\"can I have a coffee?\", \"sure, would you like it black or with milk?\"),\n",
    "    (\"thank you for the help\", \"happy to assist you anytime\"),\n",
    "    (\"are you open today?\", \"yes, we are open from 9 am to 9 pm\"),\n",
    "    (\"could you help me with my order\", \"of course, what would you like to order\"),\n",
    "    (\"are there any gluten free options\", \"yes, we have several gluten free dishes available\"),\n",
    "    (\"what are today's specials\", \"today's special is grilled salmon with vegetables\"),\n",
    "    (\"how late are you open\", \"we are open until 10 pm tonight\"),\n",
    "    (\"can you recommend a dessert\", \"our chocolate lava cake is very popular\"),\n",
    "    (\"I need to change my order\", \"sure, what changes would you like to make\"),\n",
    "    (\"do you deliver\", \"yes, we deliver within a 5 mile radius\"),\n",
    "    (\"what payment methods do you accept\", \"we accept cash, credit cards, and mobile payments\"),\n",
    "    (\"is there a parking facility\", \"yes, free parking is available behind the restaurant\"),\n",
    "    (\"thank you very much\", \"you're welcome, happy to help\"),\n",
    "    (\"I have a food allergy\", \"please let us know your allergy, and we will accommodate\"),\n",
    "    (\"can I book a table\", \"yes, for how many people and what time\"),\n",
    "    (\"what's your restaurant address\", \"we are located at 123 Main Street\"),\n",
    "    (\"do you have vegan meals\", \"yes, we offer delicious vegan options\"),\n",
    "    (\"can I get nutritional information\", \"nutritional info is available on our website\"),\n",
    "    (\"how long is the wait time\", \"usually about 15 minutes during peak hours\"),\n",
    "    (\"do you have a kids menu\", \"yes, we have a special menu for children\"),\n",
    "    (\"can I cancel my order\", \"please provide your order number to cancel\"),\n",
    "    (\"what are your opening hours\", \"we are open from 9 am to 10 pm daily\"),\n",
    "    (\"is takeout available\", \"yes, you can order takeout anytime during opening hours\"),        \n",
    "]\n",
    "\n",
    "# Build vocabulary\n",
    "all_texts = [t[0] + \" \" + t[1] for t in train_data]\n",
    "all_words = set(word for sentence in all_texts for word in sentence.lower().split())\n",
    "word2idx = {w: i + 1 for i, w in enumerate(sorted(all_words))}\n",
    "idx2word = np.array(['<pad>'] + sorted(all_words))\n",
    "vocab_size = len(idx2word)\n",
    "\n",
    "max_input_len = 6\n",
    "max_resp_len = 8\n",
    "\n",
    "def encode_sentence(sent, max_len):\n",
    "    words = sent.lower().split()\n",
    "    seq = [word2idx.get(w, 0) for w in words]\n",
    "    seq = seq[:max_len] + [0] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "X_input = np.array([encode_sentence(t[0], max_input_len) for t in train_data])\n",
    "X_resp_in = np.array([encode_sentence(t[1], max_resp_len) for t in train_data])\n",
    "X_resp_out = np.array([encode_sentence(t[1], max_resp_len)[1:] + [0] for t in train_data]) # shifted\n",
    "\n",
    "class Expert(Model):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense2 = Dense(d_model)\n",
    "    def call(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "class GatingNetwork(Model):\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super().__init__()\n",
    "        self.dense = Dense(num_experts)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "    def call(self, x):\n",
    "        logits = self.dense(x)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "class MoEResponseGenerator(Model):\n",
    "    def __init__(self, vocab_size, d_model, num_experts, max_resp_len, lstm_units=128):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = [Expert(d_model) for _ in range(num_experts)]\n",
    "        self.gating_net = GatingNetwork(d_model, num_experts)\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        self.to_h = Dense(lstm_units)\n",
    "        self.to_c = Dense(lstm_units)\n",
    "        self.output_layer = Dense(vocab_size)\n",
    "        self.max_resp_len = max_resp_len\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_seq, resp_in_seq = inputs\n",
    "        enc_emb = self.embedding(input_seq)\n",
    "        pooled = self.global_pool(enc_emb)\n",
    "\n",
    "        gating_probs = self.gating_net(pooled)\n",
    "        expert_outs = tf.stack([expert(pooled) for expert in self.experts], axis=1)\n",
    "        gated_rep = tf.reduce_sum(tf.expand_dims(gating_probs, 2) * expert_outs, axis=1)\n",
    "\n",
    "        h_state = self.to_h(gated_rep)\n",
    "        c_state = self.to_c(gated_rep)\n",
    "\n",
    "        resp_emb = self.embedding(resp_in_seq)\n",
    "        lstm_out, _, _ = self.lstm(resp_emb, initial_state=[h_state, c_state])\n",
    "\n",
    "        logits = self.output_layer(lstm_out)\n",
    "        return logits, gating_probs\n",
    "\n",
    "# Hyperparams and dataset\n",
    "d_model = 32\n",
    "num_experts = 4\n",
    "batch_size = 2\n",
    "epochs = 250\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((X_input, X_resp_in), X_resp_out)).shuffle(20).batch(batch_size)\n",
    "\n",
    "model = MoEResponseGenerator(vocab_size, d_model, num_experts, max_resp_len)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, _ = model(inputs, training=True)\n",
    "        loss = loss_fn(labels, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for in_batch, out_batch in dataset:\n",
    "        loss = train_step(in_batch, out_batch)\n",
    "        total_loss += loss.numpy()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Loss: {total_loss/len(dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_logits(logits, temperature=1.0, top_k=5):\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        values, _ = tf.math.top_k(logits, k=top_k)\n",
    "        min_values = values[:, -1, None]\n",
    "        logits = tf.where(\n",
    "            logits < min_values,\n",
    "            tf.fill(tf.shape(logits), float('-inf')),\n",
    "            logits,\n",
    "        )\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    next_token = tf.random.categorical(tf.math.log(probabilities), num_samples=1)\n",
    "    return tf.squeeze(next_token, axis=-1).numpy()\n",
    "\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    input_seq = np.array([encode_sentence(input_text, max_input_len)])\n",
    "    response_seq = np.zeros((1, max_resp_len), dtype=np.int32)\n",
    "    generated_tokens = []\n",
    "    gating_probs = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        logits, gating_probs = model((input_seq, response_seq), training=False)\n",
    "        logits_step = logits[:, i % max_resp_len, :]\n",
    "        \n",
    "        next_token = sample_from_logits(logits_step, temperature=temperature, top_k=top_k)[0]\n",
    "\n",
    "        if next_token == 0:  # end on padding token\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(idx2word[next_token])\n",
    "        if i + 1 < max_resp_len:\n",
    "            response_seq[0, i] = next_token\n",
    "        else:\n",
    "            response_seq = np.roll(response_seq, -1, axis=1)\n",
    "            response_seq[0, -1] = next_token\n",
    "\n",
    "    top_expert = np.argmax(gating_probs[0].numpy()) if gating_probs is not None else -1\n",
    "    return \" \".join(generated_tokens), top_expert, gating_probs[0].numpy()\n",
    "\n",
    "\n",
    "# Demo\n",
    "inference_prompts = [\n",
    "    \"hello there\",\n",
    "    \"I want to order pizza\",\n",
    "    \"goodbye\",\n",
    "    \"can you help me order food\",\n",
    "    \"hi, what's going on?\",\n",
    "    \"will it rain tomorrow?\",\n",
    "    \"how do I say goodbye politely?\",\n",
    "    \"what toppings do you have?\",\n",
    "    \"is today sunny or cloudy?\",\n",
    "    \"see you soon\"\n",
    "]\n",
    "for input_text in inference_prompts:\n",
    "    response, expert_used, gating_distribution = generate_response(model, input_text, max_len=500)\n",
    "\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Generated response:\", response)\n",
    "    print(\"Top expert used:\", expert_used)\n",
    "    print(\"Gating probabilities:\", np.round(gating_distribution, 3))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_array = np.array(image)\n",
    "    mean_colors = image_array.mean(axis=(0, 1))\n",
    "    color_names = ['red', 'green', 'blue']\n",
    "    dominant_color = color_names[np.argmax(mean_colors)]\n",
    "    return f\"Dominant color is {dominant_color}.\"\n",
    "\n",
    "def multimodal_response(model, input_text=None, image_path=None):\n",
    "    result = \"\"\n",
    "    if input_text:\n",
    "        # Use your generate_response logic for text\n",
    "        response, exp_used, gating_distribution = generate_response(model, input_text, max_len=30)\n",
    "        result += f\"Text response: {response}\\nTop expert used: {exp_used}\\n\"\n",
    "    if image_path:\n",
    "        # Classify the image\n",
    "        image_result = classify_image(image_path)\n",
    "        result += f\"Image analysis: {image_result}\\n\"\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "print(multimodal_response(model, input_text=\"what toppings do you have?\", image_path=\"sample_image.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Softmax, LSTM, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# ----------- MNIST DIGIT CLASSIFIER -----------\n",
    "\n",
    "class DigitClassifier(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(32, (3,3), activation='relu')\n",
    "        self.pool1 = MaxPooling2D((2,2))\n",
    "        self.conv2 = Conv2D(64, (3,3), activation='relu')\n",
    "        self.pool2 = MaxPooling2D((2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(64, activation='relu')\n",
    "        self.out = Dense(10)  # digits 0-9 logits\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return self.out(x)\n",
    "\n",
    "def prepare_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# Train or load a pretrained digit classifier\n",
    "digit_classifier = DigitClassifier()\n",
    "(x_train, y_train), (x_test, y_test) = prepare_mnist_data()\n",
    "digit_classifier.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "print(\"Training digit classifier (this may take a bit)...\")\n",
    "digit_classifier.fit(x_train, y_train, epochs=3, batch_size=128, validation_split=0.1)\n",
    "\n",
    "# ----------- YOUR MOE TEXT GENERATION MODEL (copy your existing) -----------\n",
    "\n",
    "# (Paste your current MoEResponseGenerator, Expert, GatingNetwork, encode_sentence, sample_from_logits, etc. here)\n",
    "# For brevity, let's assume it's loaded as `moe_model`\n",
    "\n",
    "# For this example, just a placeholder generate_response:\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    # Use your real model's generation logic here\n",
    "    return f\"Simulated response to: {input_text}\", 1, np.array([0.1, 0.7, 0.1, 0.1])\n",
    "\n",
    "# ----------- MNIST INCREMENT LOGIC -----------\n",
    "\n",
    "def get_incremented_digit_image(input_image, x_dataset, y_dataset):\n",
    "    # Normalize pixel values and ensure float32\n",
    "    img = input_image.astype('float32')\n",
    "    if img.max() > 1.0:\n",
    "        img /= 255.0\n",
    "\n",
    "    # Add channel dimension if missing\n",
    "    if len(img.shape) == 2:   # grayscale image shape (28,28)\n",
    "        img = np.expand_dims(img, axis=-1)  # become (28,28,1)\n",
    "\n",
    "    # Add batch dimension for model input\n",
    "    img = np.expand_dims(img, axis=0)  # become (1,28,28,1)\n",
    "\n",
    "    logits = digit_classifier(img)\n",
    "    pred_digit = tf.argmax(logits, axis=1).numpy()[0]\n",
    "\n",
    "    inc_digit = 0 if pred_digit == 9 else pred_digit + 1\n",
    "    idx = np.where(y_dataset == inc_digit)[0][0]\n",
    "    inc_image = x_dataset[idx][:, :, 0]  # remove channel for display\n",
    "\n",
    "    return pred_digit, inc_digit, inc_image\n",
    "\n",
    "\n",
    "# ----------- MULTIMODAL HANDLER -----------\n",
    "\n",
    "def multimodal_handler(input_text=None, input_image=None):\n",
    "    if input_text:\n",
    "        response, expert, gating = generate_response(None, input_text)\n",
    "        print(f\"Text Input: {input_text}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Top expert used: {expert}\")\n",
    "        print(f\"Gating probabilities: {gating}\")\n",
    "    if input_image is not None:\n",
    "        pred_digit, inc_digit, inc_image = get_incremented_digit_image(input_image, x_test, np.argmax(y_test, axis=1))\n",
    "        print(f\"Input Digit: {pred_digit}, Incremented Digit: {inc_digit}\")\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title(f\"Input: {pred_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(inc_image, cmap='gray')\n",
    "        plt.title(f\"Output: {inc_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# ----------- DEMO -----------\n",
    "\n",
    "# Text-only\n",
    "multimodal_handler(input_text=\"I need help with my order.\")\n",
    "\n",
    "# Image-only\n",
    "multimodal_handler(input_image=x_test[5])\n",
    "\n",
    "# Both modalities\n",
    "multimodal_handler(input_text=\"What digit is this?\", input_image=x_test[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_3.10_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
