{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816ms/step - accuracy: 0.5000 - loss: 1.5911\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 1.4788\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.3847\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 1.2850\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 1.2736\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 1.0831\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.9123\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.9072\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.9243\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.7506\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.5848\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.6014\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.4090\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.4189\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.3150\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.2970\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.1980\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.1673\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.2163\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.1631\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.1098\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0604\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0491\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0585\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0461\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.1271\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0442\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0315\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0270\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0181\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0203\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0260\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0061\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0090\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0120\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0032\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0173\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 9.5097e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0062\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0082\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 6.3131e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 6.1573e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 7.0068e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 7.0222e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 7.1259e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 7.8480e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 9.1718e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 5.4471e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 4.9477e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 5.4875e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 6.0937e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 5.6070e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.2117e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 2.9493e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.0477e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 5.0100e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 3.7722e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.5752e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 4.3442e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 9.4887e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 5.8307e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 5.0314e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 6.8093e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.9254e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 8.5562e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 7.3857e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 7.5819e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 4.6986e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 4.6676e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 3.5512e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 2.8987e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 4.0104e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.3893e-04\n",
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.0000e+00 - loss: 1.5905\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 1.5587\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 1.5336\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 1.5110\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0000e+00 - loss: 1.5827\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 1.4895\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5000 - loss: 1.4769\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 1.4469\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 1.4534\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 1.3728\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 1.3796\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 1.3537\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 1.3139\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5000 - loss: 1.3698\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 1.3189\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.3049\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 1.2689\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 1.2688\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 1.2509\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5000 - loss: 1.2292\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5000 - loss: 1.1100\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 1.1682\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 1.1580\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 1.1293\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 1.0449\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 1.0277\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 1.0031\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 1.0594\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.9965\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.9164\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.9140\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 0.8763\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 1.0193\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.8889\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.8758\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 0.8750\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 0.8279\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.8797\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.8134\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 0.8614\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 0.8753\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.8019\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 0.7615\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5000 - loss: 0.7457\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.7745\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 0.7641\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 0.7268\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 0.7523\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.7162\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.7333\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 0.7158\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.7131\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.7074\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.7232\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.6894\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.6959\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.7472\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5000 - loss: 0.7439\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.6446\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 0.7068\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.6865\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.6512\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.6785\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.6224\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.6609\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.5929\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.6409\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.6163\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.6401\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.6419\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.6168\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.6104\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.5936\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.6337\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.5374\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.5469\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.5467\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.5534\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.4848\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.5368\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.5383\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.5161\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.6300\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.4826\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.5896\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.4037\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.4893\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.4768\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.4715\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.3852\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.4633\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.5299\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.3938\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.4040\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.4853\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.4116\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.2812\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.3084\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.3129\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.2713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x31ef68f40>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Output vocab (for targets and predictions)\n",
    "out_vocab = {\"<pad>\":0,\"cat\":1,\"dog\":2,\"meow\":3,\"bark\":4}\n",
    "inv_out_vocab = {v:k for k,v in out_vocab.items()}\n",
    "\n",
    "def encode_output_seq(seq, max_len=1):\n",
    "    tokens = seq.split()\n",
    "    ids = [out_vocab[w] for w in tokens]\n",
    "    ids += [0]*(max_len-len(ids))\n",
    "    return np.array(ids)\n",
    "\n",
    "def decode(ids):\n",
    "    return \" \".join(inv_out_vocab[i] for i in ids if i>0)\n",
    "\n",
    "class HubEmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hub_url=\"https://tfhub.dev/google/universal-sentence-encoder/4\"):\n",
    "        super().__init__()\n",
    "        self.hub_layer = hub.KerasLayer(hub_url, trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.hub_layer(inputs)\n",
    "\n",
    "\n",
    "def build_text_expert():\n",
    "    inp = layers.Input(shape=(), dtype=tf.string, name=\"text_in\")\n",
    "    embedding_layer = HubEmbeddingLayer()  # default is USE 512-d\n",
    "    x = embedding_layer(inp)\n",
    "    x = layers.Dense(512, activation='relu')(x)        # increased capacity\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(len(out_vocab), activation='softmax')(x)\n",
    "    return Model(inp, out, name=\"TextExpert\")\n",
    "\n",
    "def build_image_expert():\n",
    "    inp = layers.Input(shape=(4,4,1), name=\"image_in\")\n",
    "    x = layers.Conv2D(16, (2,2), activation='relu')(inp)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(len(out_vocab), activation='softmax')(x)\n",
    "    return Model(inp, out, name=\"ImageExpert\")\n",
    "\n",
    "text_expert = build_text_expert()\n",
    "image_expert = build_image_expert()\n",
    "\n",
    "initial_lr = 1e-3\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "print(\"Compiling models...\")\n",
    "text_expert.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "image_expert.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "text_inputs = [\"cat\", \"dog\"]\n",
    "text_targets = [\"meow\", \"bark\"]\n",
    "\n",
    "image_inputs = np.array([np.ones((4,4)), np.zeros((4,4))])\n",
    "image_targets = [\"cat\", \"dog\"]\n",
    "\n",
    "X_text_in = tf.constant(text_inputs, dtype=tf.string)\n",
    "Y_text_out = np.array([out_vocab[t] for t in text_targets])\n",
    "X_img_in = image_inputs[..., None]\n",
    "Y_img_out = np.array([out_vocab[t] for t in image_targets])\n",
    "\n",
    "print(\"Train text expert...\")\n",
    "text_expert.fit(X_text_in, Y_text_out, epochs=100, verbose=1)\n",
    "\n",
    "print(\"Train image expert...\")\n",
    "image_expert.fit(X_img_in, Y_img_out, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: dog, Probabilities: [[6.7759820e-06 1.6639984e-05 3.5206945e-06 3.2574616e-04 9.9964726e-01]]\n",
      "Text input 'dog'     => bark\n",
      "Input: canine, Probabilities: [[1.64616809e-04 3.40726372e-04 1.06246895e-04 3.48554272e-03\n",
      "  9.95902836e-01]]\n",
      "Text input 'canine'  => bark\n",
      "Input: wolf, Probabilities: [[0.00292729 0.00583772 0.0015113  0.18139388 0.80832976]]\n",
      "Text input 'wolf'    => bark\n",
      "Input: labrador, Probabilities: [[1.1411209e-03 1.9138358e-03 7.9245231e-04 1.0606636e-02 9.8554593e-01]]\n",
      "Text input 'labrador'    => bark\n",
      "Input: coyote, Probabilities: [[0.0075675  0.01140293 0.00480268 0.08793327 0.8882936 ]]\n",
      "Text input 'coyote'  => bark\n",
      "Input: puppy, Probabilities: [[1.00294594e-04 2.29717669e-04 5.90392046e-05 4.15754551e-03\n",
      "  9.95453358e-01]]\n",
      "Text input 'puppy'  => bark\n",
      "Input: cat, Probabilities: [[1.0049924e-05 4.3694115e-05 1.9005014e-06 9.9954093e-01 4.0346046e-04]]\n",
      "Text input 'cat'     => meow\n",
      "Input: feline, Probabilities: [[3.45496868e-04 1.03776553e-03 1.12378846e-04 9.89282012e-01\n",
      "  9.22233984e-03]]\n",
      "Text input 'feline'  => meow\n",
      "Input: ragdoll, Probabilities: [[0.00558097 0.01104869 0.0028128  0.9462539  0.03430356]]\n",
      "Text input 'ragdoll'     => meow\n",
      "Input: lion, Probabilities: [[0.00752297 0.01445793 0.00424143 0.42027634 0.55350137]]\n",
      "Text input 'lion'  => bark\n",
      "Input: tiger, Probabilities: [[0.00480601 0.01021975 0.00233285 0.84174454 0.14089683]]\n",
      "Text input 'tiger'  => meow\n",
      "Input: cat, Probabilities: [[1.0049924e-05 4.3694115e-05 1.9005014e-06 9.9954093e-01 4.0346046e-04]]\n",
      "Cat-like image       => ('cat', 'meow')\n",
      "Input: dog, Probabilities: [[6.7759820e-06 1.6639984e-05 3.5206945e-06 3.2574616e-04 9.9964726e-01]]\n",
      "Dog-like image       => ('dog', 'bark')\n"
     ]
    }
   ],
   "source": [
    "# -------- Inference functions --------\n",
    "def predict_from_text(word):\n",
    "    preds = text_expert.predict(tf.constant([word]), verbose=0)\n",
    "    print(f\"Input: {word}, Probabilities: {preds}\")\n",
    "    pred_id = np.argmax(preds[0])\n",
    "    return inv_out_vocab[pred_id]\n",
    "\n",
    "def predict_from_image(img):\n",
    "    # Step 1: predict animal from image\n",
    "    animal_probs = image_expert.predict(img[None,...,None], verbose=0)\n",
    "    animal_id = np.argmax(animal_probs[0])\n",
    "    animal = inv_out_vocab[animal_id]\n",
    "    # Step 2: predict sound from text expert using animal label\n",
    "    sound = predict_from_text(animal)\n",
    "    return animal, sound\n",
    "\n",
    "# -------- Testing --------\n",
    "print(\"Text input 'dog'     =>\", predict_from_text(\"dog\"))\n",
    "print(\"Text input 'canine'  =>\", predict_from_text(\"canine\"))\n",
    "print(\"Text input 'wolf'    =>\", predict_from_text(\"wolf\"))\n",
    "print(\"Text input 'labrador'    =>\", predict_from_text(\"labrador\"))\n",
    "print(\"Text input 'coyote'  =>\", predict_from_text(\"coyote\"))\n",
    "print(\"Text input 'puppy'  =>\", predict_from_text(\"puppy\"))\n",
    "print(\"Text input 'cat'     =>\", predict_from_text(\"cat\"))\n",
    "print(\"Text input 'feline'  =>\", predict_from_text(\"feline\"))\n",
    "print(\"Text input 'ragdoll'     =>\", predict_from_text(\"ragdoll\"))\n",
    "print(\"Text input 'lion'  =>\", predict_from_text(\"lion\"))\n",
    "print(\"Text input 'tiger'  =>\", predict_from_text(\"tiger\"))\n",
    "\n",
    "\n",
    "print(\"Cat-like image       =>\", predict_from_image(np.ones((4,4))))\n",
    "print(\"Dog-like image       =>\", predict_from_image(np.zeros((4,4))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Softmax, LSTM, Conv2D, Flatten, MaxPooling2D, LayerNormalization, Dropout, Input, Conv2DTranspose, Reshape, Add, Lambda, BatchNormalization, add, Activation\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# MoE Text Generation Model Components\n",
    "# ---------------------------------------------\n",
    "\n",
    "class Expert(Model):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.dense2 = Dense(d_model)\n",
    "    def call(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "class GatingNetwork(Model):\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super().__init__()\n",
    "        self.dense = Dense(num_experts)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "    def call(self, x):\n",
    "        logits = self.dense(x)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "class MoEResponseGenerator(Model):\n",
    "    def __init__(self, vocab_size, d_model, num_experts, max_resp_len, lstm_units=128):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = [Expert(d_model) for _ in range(num_experts)]\n",
    "        self.gating_net = GatingNetwork(d_model, num_experts)\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        self.to_h = Dense(lstm_units)\n",
    "        self.to_c = Dense(lstm_units)\n",
    "        self.output_layer = Dense(vocab_size)\n",
    "        self.max_resp_len = max_resp_len\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_seq, resp_in_seq = inputs\n",
    "        enc_emb = self.embedding(input_seq)\n",
    "        pooled = self.global_pool(enc_emb)\n",
    "\n",
    "        gating_probs = self.gating_net(pooled)\n",
    "        expert_outs = tf.stack([expert(pooled) for expert in self.experts], axis=1)\n",
    "        gated_rep = tf.reduce_sum(tf.expand_dims(gating_probs, 2) * expert_outs, axis=1)\n",
    "\n",
    "        h_state = self.to_h(gated_rep)\n",
    "        c_state = self.to_c(gated_rep)\n",
    "\n",
    "        resp_emb = self.embedding(resp_in_seq)\n",
    "        lstm_out, _, _ = self.lstm(resp_emb, initial_state=[h_state, c_state])\n",
    "\n",
    "        logits = self.output_layer(lstm_out)\n",
    "        return logits, gating_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Utility functions for text encoding and generation\n",
    "# ---------------------------------------------\n",
    "\n",
    "# (Your train_data list is assumed defined here - omitted for brevity, but same as your list)\n",
    "\n",
    "# Build vocabulary based on train_data (input + responses)\n",
    "train_data = [\n",
    "    (\"hello there\", \"hi, how can I help\"),\n",
    "    (\"hi\", \"hello, what can I do\"),\n",
    "    (\"goodbye\", \"goodbye, have a nice day\"),\n",
    "    (\"see you later\", \"see you soon, goodbye\"),\n",
    "    (\"I want to order pizza\", \"sure, what toppings do you want\"),\n",
    "    (\"can I get a burger\", \"what size burger would you like\"),\n",
    "    (\"what is the weather\", \"the weather today is sunny\"),\n",
    "    (\"is it raining\", \"no rain expected today\"),\n",
    "    (\"hey, I want some pasta\", \"what kind of pasta would you prefer\"),\n",
    "    (\"do you have vegetarian options?\", \"yes, we have several vegetarian dishes\"),\n",
    "    (\"good morning\", \"good morning, how may I assist you?\"),\n",
    "    (\"bye\", \"take care, see you later\"),\n",
    "    (\"will it be hot today?\", \"expect warm temperatures all day\"),\n",
    "    (\"can I order a salad?\", \"what dressing would you like on your salad?\"),\n",
    "    (\"thanks, goodbye\", \"you're welcome, goodbye!\"),\n",
    "    (\"tell me the forecast\", \"the forecast shows clear skies\"),\n",
    "    (\"what's your name?\", \"i am your assistant, here to help\"),\n",
    "    (\"can I have a coffee?\", \"sure, would you like it black or with milk?\"),\n",
    "    (\"thank you for the help\", \"happy to assist you anytime\"),\n",
    "    (\"are you open today?\", \"yes, we are open from 9 am to 9 pm\"),\n",
    "    (\"could you help me with my order\", \"of course, what would you like to order\"),\n",
    "    (\"are there any gluten free options\", \"yes, we have several gluten free dishes available\"),\n",
    "    (\"what are today's specials\", \"today's special is grilled salmon with vegetables\"),\n",
    "    (\"how late are you open\", \"we are open until 10 pm tonight\"),\n",
    "    (\"can you recommend a dessert\", \"our chocolate lava cake is very popular\"),\n",
    "    (\"I need to change my order\", \"sure, what changes would you like to make\"),\n",
    "    (\"do you deliver\", \"yes, we deliver within a 5 mile radius\"),\n",
    "    (\"what payment methods do you accept\", \"we accept cash, credit cards, and mobile payments\"),\n",
    "    (\"is there a parking facility\", \"yes, free parking is available behind the restaurant\"),\n",
    "    (\"thank you very much\", \"you're welcome, happy to help\"),\n",
    "    (\"I have a food allergy\", \"please let us know your allergy, and we will accommodate\"),\n",
    "    (\"can I book a table\", \"yes, for how many people and what time\"),\n",
    "    (\"what's your restaurant address\", \"we are located at 123 Main Street\"),\n",
    "    (\"do you have vegan meals\", \"yes, we offer delicious vegan options\"),\n",
    "    (\"can I get nutritional information\", \"nutritional info is available on our website\"),\n",
    "    (\"how long is the wait time\", \"usually about 15 minutes during peak hours\"),\n",
    "    (\"do you have a kids menu\", \"yes, we have a special menu for children\"),\n",
    "    (\"can I cancel my order\", \"please provide your order number to cancel\"),\n",
    "    (\"what are your opening hours\", \"we are open from 9 am to 10 pm daily\"),\n",
    "    (\"is takeout available\", \"yes, you can order takeout anytime during opening hours\"),        \n",
    "]\n",
    "\n",
    "all_texts = [t[0] + \" \" + t[1] for t in train_data]\n",
    "all_words = set(word for sentence in all_texts for word in sentence.lower().split())\n",
    "word2idx = {w: i + 1 for i, w in enumerate(sorted(all_words))}\n",
    "idx2word = np.array(['<pad>'] + sorted(all_words))\n",
    "vocab_size = len(idx2word)\n",
    "max_input_len = 6\n",
    "max_resp_len = 8\n",
    "\n",
    "def encode_sentence(sent, max_len):\n",
    "    words = sent.lower().split()\n",
    "    seq = [word2idx.get(w, 0) for w in words]\n",
    "    seq = seq[:max_len] + [0] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0, top_k=5):\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        values, _ = tf.math.top_k(logits, k=top_k)\n",
    "        min_values = values[:, -1, None]\n",
    "        logits = tf.where(\n",
    "            logits < min_values,\n",
    "            tf.fill(tf.shape(logits), float('-inf')),\n",
    "            logits,\n",
    "        )\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    next_token = tf.random.categorical(tf.math.log(probabilities), num_samples=1)\n",
    "    return tf.squeeze(next_token, axis=-1).numpy()\n",
    "\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    input_seq = np.array([encode_sentence(input_text, max_input_len)])\n",
    "    response_seq = np.zeros((1, max_resp_len), dtype=np.int32)\n",
    "    generated_tokens = []\n",
    "    gating_probs = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        logits, gating_probs = model((input_seq, response_seq), training=False)\n",
    "        logits_step = logits[:, i % max_resp_len, :]\n",
    "        next_token = sample_from_logits(logits_step, temperature=temperature, top_k=top_k)[0]\n",
    "        if next_token == 0:\n",
    "            break\n",
    "        generated_tokens.append(idx2word[next_token])\n",
    "        if i + 1 < max_resp_len:\n",
    "            response_seq[0, i] = next_token\n",
    "        else:\n",
    "            response_seq = np.roll(response_seq, -1, axis=1)\n",
    "            response_seq[0, -1] = next_token\n",
    "\n",
    "    top_expert = np.argmax(gating_probs[0].numpy()) if gating_probs is not None else -1\n",
    "    return \" \".join(generated_tokens), top_expert, gating_probs[0].numpy()\n",
    "\n",
    "# Build training dataset\n",
    "X_input = np.array([encode_sentence(t[0], max_input_len) for t in train_data])\n",
    "X_resp_in = np.array([encode_sentence(t[1], max_resp_len) for t in train_data])\n",
    "X_resp_out = np.array([encode_sentence(t[1], max_resp_len)[1:] + [0] for t in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# MNIST Digit Classifier Components\n",
    "# ---------------------------------------------\n",
    "\n",
    "class DigitClassifier(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(32, (3,3), activation='relu')\n",
    "        self.pool1 = MaxPooling2D((2,2))\n",
    "        self.conv2 = Conv2D(64, (3,3), activation='relu')\n",
    "        self.pool2 = MaxPooling2D((2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(64, activation='relu')\n",
    "        self.out = Dense(10)  # digits 0-9 logits\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return self.out(x)\n",
    "\n",
    "def prepare_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Training setup and training loops\n",
    "# ---------------------------------------------\n",
    "\n",
    "digit_classifier = DigitClassifier()\n",
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = prepare_mnist_data()\n",
    "digit_classifier.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "print(\"Training MNIST digit classifier...\")\n",
    "digit_classifier.fit(x_train_mnist, y_train_mnist, epochs=3, batch_size=128, validation_split=0.1)\n",
    "\n",
    "d_model = 32\n",
    "num_experts = 5\n",
    "lstm_units = 128\n",
    "max_epochs = 300\n",
    "batch_size = 2\n",
    "\n",
    "moe_model = MoEResponseGenerator(vocab_size, d_model, num_experts, max_resp_len, lstm_units)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((X_input, X_resp_in), X_resp_out)).shuffle(50).batch(batch_size)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, _ = moe_model(inputs, training=True)\n",
    "        loss = loss_fn(labels, logits)\n",
    "    grads = tape.gradient(loss, moe_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, moe_model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "print(\"Training MoE text generation model...\")\n",
    "for epoch in range(max_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in train_dataset:\n",
    "        loss = train_step(batch[0], batch[1])\n",
    "        total_loss += loss.numpy()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{max_epochs}: Loss = {total_loss / len(train_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "# Load and normalize MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Prepare target images for increment digits\n",
    "train_target_images = np.zeros_like(x_train)\n",
    "for idx in range(len(x_train)):\n",
    "    inc_digit = (y_train[idx] + 1) % 10\n",
    "    candidates = np.where(y_train == inc_digit)[0]\n",
    "    chosen_idx = np.random.choice(candidates)\n",
    "    train_target_images[idx] = x_train[chosen_idx]\n",
    "\n",
    "test_target_images = np.zeros_like(x_test)\n",
    "for idx in range(len(x_test)):\n",
    "    inc_digit = (y_test[idx] + 1) % 10\n",
    "    candidates = np.where(y_test == inc_digit)[0]\n",
    "    chosen_idx = np.random.choice(candidates)\n",
    "    test_target_images[idx] = x_test[chosen_idx]\n",
    "\n",
    "# Residual block\n",
    "def residual_block(x, filters, kernel_size=3):\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = add([shortcut, x])\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "# Build model with residual blocks\n",
    "def build_improved_increment_model():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    x = Conv2D(64, 3, strides=2, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x = Conv2D(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = residual_block(x, 128)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(7*7*128, activation=\"relu\")(x)\n",
    "    x = Reshape((7,7,128))(x)\n",
    "    x = Conv2DTranspose(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2DTranspose(64, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Conv2D(1, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "increment_model = build_improved_increment_model()\n",
    "\n",
    "# Initialize VGG16 model for perceptual loss - with input shape 32x32x3 (minimum required)\n",
    "vgg = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "vgg.trainable = False\n",
    "feature_extractor = Model(vgg.input, vgg.get_layer(\"block3_conv3\").output)\n",
    "\n",
    "# Preprocessing function to resize and replicate grayscale channel for VGG16\n",
    "def preprocess_for_vgg(x):\n",
    "    x_resized = tf.image.resize(x, [32, 32])\n",
    "    x_rgb = tf.image.grayscale_to_rgb(x_resized)\n",
    "    x_preprocessed = preprocess_input(x_rgb * 255.0)\n",
    "    return x_preprocessed\n",
    "\n",
    "# Perceptual loss combining MSE and VGG feature loss\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    y_true_pp = preprocess_for_vgg(y_true)\n",
    "    y_pred_pp = preprocess_for_vgg(y_pred)\n",
    "    f_true = feature_extractor(y_true_pp)\n",
    "    f_pred = feature_extractor(y_pred_pp)\n",
    "    return tf.reduce_mean(tf.square(f_true - f_pred))\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    pl = perceptual_loss(y_true, y_pred)\n",
    "    return mse + 0.1 * pl\n",
    "\n",
    "increment_model.compile(optimizer='adam', loss=combined_loss)\n",
    "\n",
    "# Train with learning rate scheduler callback\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "increment_model.fit(\n",
    "    x_train,\n",
    "    train_target_images,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_test, test_target_images),\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n",
    "\n",
    "# Inference helper\n",
    "def get_incremented_digit_image(input_image):\n",
    "    img = input_image.astype(\"float32\") / 255.0\n",
    "    if img.ndim == 2:\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    generated_img = increment_model.predict(img)\n",
    "    return generated_img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of test results for a random sample\n",
    "# Visualization test\n",
    "idx = np.random.randint(len(x_test))\n",
    "input_img = x_test[idx]\n",
    "original_digit = y_test[idx]\n",
    "inc_digit = (original_digit + 1) % 10\n",
    "generated_img = get_incremented_digit_image(input_img)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(f\"Input: {original_digit}\")\n",
    "plt.imshow(input_img[:,:,0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(f\"Generated: {inc_digit}\")\n",
    "plt.imshow(generated_img[:,:,0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(f\"Expected: {inc_digit}\")\n",
    "expected_img = x_test[np.where(y_test == inc_digit)[0][0]]\n",
    "plt.imshow(expected_img[:,:,0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_handler(input_text=None, input_image=None):\n",
    "    if input_text:\n",
    "        response, expert, gating_probs = generate_response(moe_model, input_text)\n",
    "        print(f\"Text Input: {input_text}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Top expert used: {expert}\")\n",
    "        print(f\"Gating probs: {np.round(gating_probs, 3)}\")\n",
    "    if input_image is not None:\n",
    "        pred_digit, inc_digit, inc_image = get_incremented_digit_image_generative(input_image)\n",
    "        print(f\"Image input digit: {pred_digit}, incremented output digit: {inc_digit}\")\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title(f\"Input: {pred_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(inc_image, cmap='gray')\n",
    "        plt.title(f\"Output: {inc_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Demo code to test multimodal system\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Text only\n",
    "multimodal_handler(input_text=\"hello, how are you?\")\n",
    "\n",
    "# Image only (from MNIST test)\n",
    "multimodal_handler(input_image=x_test_mnist[15])\n",
    "\n",
    "# Both\n",
    "multimodal_handler(input_text=\"What number is this?\", input_image=x_test_mnist[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_examples = [\n",
    "    \"hello\",\n",
    "    \"hi there\",\n",
    "    \"good morning\",\n",
    "    \"hey, how are you?\",\n",
    "    \"what's up?\"\n",
    "]\n",
    "\n",
    "goodbye_examples = [\n",
    "    \"goodbye\",\n",
    "    \"see you later\",\n",
    "    \"bye for now\",\n",
    "    \"talk to you soon\",\n",
    "    \"have a great day\"\n",
    "]\n",
    "\n",
    "order_food_examples = [\n",
    "    \"I want to order a pizza\",\n",
    "    \"can I get a burger please?\",\n",
    "    \"what sides do you have?\",\n",
    "    \"I'd like a vegetarian pasta\",\n",
    "    \"do you have gluten free options?\"\n",
    "]\n",
    "\n",
    "weather_examples = [\n",
    "    \"what's the weather today?\",\n",
    "    \"will it rain tomorrow?\",\n",
    "    \"is it sunny outside?\",\n",
    "    \"what's the forecast for this week?\",\n",
    "    \"do I need an umbrella today?\"\n",
    "]\n",
    "\n",
    "miscellaneous_examples = [\n",
    "    \"what's your name?\",\n",
    "    \"can you tell me a joke?\",\n",
    "    \"how do I say goodbye politely?\",\n",
    "    \"are you open on weekends?\",\n",
    "    \"what time do you close?\"\n",
    "]\n",
    "\n",
    "inference_prompts = greeting_examples + goodbye_examples + order_food_examples + weather_examples + miscellaneous_examples\n",
    "for input_text in inference_prompts:\n",
    "    multimodal_handler(input_text)\n",
    "\n",
    "    # print(\"Input:\", input_text)\n",
    "    # print(\"Generated response:\", response)\n",
    "    # print(\"Top expert used:\", expert_used)\n",
    "    # print(\"Gating probabilities:\", np.round(gating_distribution, 3))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (input, response)\n",
    "train_data = [\n",
    "    (\"hello there\", \"hi, how can I help\"),\n",
    "    (\"hi\", \"hello, what can I do\"),\n",
    "    (\"goodbye\", \"goodbye, have a nice day\"),\n",
    "    (\"see you later\", \"see you soon, goodbye\"),\n",
    "    (\"I want to order pizza\", \"sure, what toppings do you want\"),\n",
    "    (\"can I get a burger\", \"what size burger would you like\"),\n",
    "    (\"what is the weather\", \"the weather today is sunny\"),\n",
    "    (\"is it raining\", \"no rain expected today\"),\n",
    "    (\"hey, I want some pasta\", \"what kind of pasta would you prefer\"),\n",
    "    (\"do you have vegetarian options?\", \"yes, we have several vegetarian dishes\"),\n",
    "    (\"good morning\", \"good morning, how may I assist you?\"),\n",
    "    (\"bye\", \"take care, see you later\"),\n",
    "    (\"will it be hot today?\", \"expect warm temperatures all day\"),\n",
    "    (\"can I order a salad?\", \"what dressing would you like on your salad?\"),\n",
    "    (\"thanks, goodbye\", \"you're welcome, goodbye!\"),\n",
    "    (\"tell me the forecast\", \"the forecast shows clear skies\"),\n",
    "    (\"what's your name?\", \"i am your assistant, here to help\"),\n",
    "    (\"can I have a coffee?\", \"sure, would you like it black or with milk?\"),\n",
    "    (\"thank you for the help\", \"happy to assist you anytime\"),\n",
    "    (\"are you open today?\", \"yes, we are open from 9 am to 9 pm\"),\n",
    "    (\"could you help me with my order\", \"of course, what would you like to order\"),\n",
    "    (\"are there any gluten free options\", \"yes, we have several gluten free dishes available\"),\n",
    "    (\"what are today's specials\", \"today's special is grilled salmon with vegetables\"),\n",
    "    (\"how late are you open\", \"we are open until 10 pm tonight\"),\n",
    "    (\"can you recommend a dessert\", \"our chocolate lava cake is very popular\"),\n",
    "    (\"I need to change my order\", \"sure, what changes would you like to make\"),\n",
    "    (\"do you deliver\", \"yes, we deliver within a 5 mile radius\"),\n",
    "    (\"what payment methods do you accept\", \"we accept cash, credit cards, and mobile payments\"),\n",
    "    (\"is there a parking facility\", \"yes, free parking is available behind the restaurant\"),\n",
    "    (\"thank you very much\", \"you're welcome, happy to help\"),\n",
    "    (\"I have a food allergy\", \"please let us know your allergy, and we will accommodate\"),\n",
    "    (\"can I book a table\", \"yes, for how many people and what time\"),\n",
    "    (\"what's your restaurant address\", \"we are located at 123 Main Street\"),\n",
    "    (\"do you have vegan meals\", \"yes, we offer delicious vegan options\"),\n",
    "    (\"can I get nutritional information\", \"nutritional info is available on our website\"),\n",
    "    (\"how long is the wait time\", \"usually about 15 minutes during peak hours\"),\n",
    "    (\"do you have a kids menu\", \"yes, we have a special menu for children\"),\n",
    "    (\"can I cancel my order\", \"please provide your order number to cancel\"),\n",
    "    (\"what are your opening hours\", \"we are open from 9 am to 10 pm daily\"),\n",
    "    (\"is takeout available\", \"yes, you can order takeout anytime during opening hours\"),        \n",
    "]\n",
    "\n",
    "# Build vocabulary\n",
    "all_texts = [t[0] + \" \" + t[1] for t in train_data]\n",
    "all_words = set(word for sentence in all_texts for word in sentence.lower().split())\n",
    "word2idx = {w: i + 1 for i, w in enumerate(sorted(all_words))}\n",
    "idx2word = np.array(['<pad>'] + sorted(all_words))\n",
    "vocab_size = len(idx2word)\n",
    "\n",
    "max_input_len = 6\n",
    "max_resp_len = 8\n",
    "\n",
    "def encode_sentence(sent, max_len):\n",
    "    words = sent.lower().split()\n",
    "    seq = [word2idx.get(w, 0) for w in words]\n",
    "    seq = seq[:max_len] + [0] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "X_input = np.array([encode_sentence(t[0], max_input_len) for t in train_data])\n",
    "X_resp_in = np.array([encode_sentence(t[1], max_resp_len) for t in train_data])\n",
    "X_resp_out = np.array([encode_sentence(t[1], max_resp_len)[1:] + [0] for t in train_data]) # shifted\n",
    "\n",
    "class Expert(Model):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense2 = Dense(d_model)\n",
    "    def call(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "class GatingNetwork(Model):\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super().__init__()\n",
    "        self.dense = Dense(num_experts)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "    def call(self, x):\n",
    "        logits = self.dense(x)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "class MoEResponseGenerator(Model):\n",
    "    def __init__(self, vocab_size, d_model, num_experts, max_resp_len, lstm_units=128):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = [Expert(d_model) for _ in range(num_experts)]\n",
    "        self.gating_net = GatingNetwork(d_model, num_experts)\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        self.to_h = Dense(lstm_units)\n",
    "        self.to_c = Dense(lstm_units)\n",
    "        self.output_layer = Dense(vocab_size)\n",
    "        self.max_resp_len = max_resp_len\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_seq, resp_in_seq = inputs\n",
    "        enc_emb = self.embedding(input_seq)\n",
    "        pooled = self.global_pool(enc_emb)\n",
    "\n",
    "        gating_probs = self.gating_net(pooled)\n",
    "        expert_outs = tf.stack([expert(pooled) for expert in self.experts], axis=1)\n",
    "        gated_rep = tf.reduce_sum(tf.expand_dims(gating_probs, 2) * expert_outs, axis=1)\n",
    "\n",
    "        h_state = self.to_h(gated_rep)\n",
    "        c_state = self.to_c(gated_rep)\n",
    "\n",
    "        resp_emb = self.embedding(resp_in_seq)\n",
    "        lstm_out, _, _ = self.lstm(resp_emb, initial_state=[h_state, c_state])\n",
    "\n",
    "        logits = self.output_layer(lstm_out)\n",
    "        return logits, gating_probs\n",
    "\n",
    "# Hyperparams and dataset\n",
    "d_model = 32\n",
    "num_experts = 4\n",
    "batch_size = 2\n",
    "epochs = 250\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((X_input, X_resp_in), X_resp_out)).shuffle(20).batch(batch_size)\n",
    "\n",
    "model = MoEResponseGenerator(vocab_size, d_model, num_experts, max_resp_len)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, _ = model(inputs, training=True)\n",
    "        loss = loss_fn(labels, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for in_batch, out_batch in dataset:\n",
    "        loss = train_step(in_batch, out_batch)\n",
    "        total_loss += loss.numpy()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Loss: {total_loss/len(dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_logits(logits, temperature=1.0, top_k=5):\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        values, _ = tf.math.top_k(logits, k=top_k)\n",
    "        min_values = values[:, -1, None]\n",
    "        logits = tf.where(\n",
    "            logits < min_values,\n",
    "            tf.fill(tf.shape(logits), float('-inf')),\n",
    "            logits,\n",
    "        )\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    next_token = tf.random.categorical(tf.math.log(probabilities), num_samples=1)\n",
    "    return tf.squeeze(next_token, axis=-1).numpy()\n",
    "\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    input_seq = np.array([encode_sentence(input_text, max_input_len)])\n",
    "    response_seq = np.zeros((1, max_resp_len), dtype=np.int32)\n",
    "    generated_tokens = []\n",
    "    gating_probs = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        logits, gating_probs = model((input_seq, response_seq), training=False)\n",
    "        logits_step = logits[:, i % max_resp_len, :]\n",
    "        \n",
    "        next_token = sample_from_logits(logits_step, temperature=temperature, top_k=top_k)[0]\n",
    "\n",
    "        if next_token == 0:  # end on padding token\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(idx2word[next_token])\n",
    "        if i + 1 < max_resp_len:\n",
    "            response_seq[0, i] = next_token\n",
    "        else:\n",
    "            response_seq = np.roll(response_seq, -1, axis=1)\n",
    "            response_seq[0, -1] = next_token\n",
    "\n",
    "    top_expert = np.argmax(gating_probs[0].numpy()) if gating_probs is not None else -1\n",
    "    return \" \".join(generated_tokens), top_expert, gating_probs[0].numpy()\n",
    "\n",
    "\n",
    "# Demo\n",
    "inference_prompts = [\n",
    "    \"hello there\",\n",
    "    \"I want to order pizza\",\n",
    "    \"goodbye\",\n",
    "    \"can you help me order food\",\n",
    "    \"hi, what's going on?\",\n",
    "    \"will it rain tomorrow?\",\n",
    "    \"how do I say goodbye politely?\",\n",
    "    \"what toppings do you have?\",\n",
    "    \"is today sunny or cloudy?\",\n",
    "    \"see you soon\"\n",
    "]\n",
    "for input_text in inference_prompts:\n",
    "    response, expert_used, gating_distribution = generate_response(model, input_text, max_len=500)\n",
    "\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Generated response:\", response)\n",
    "    print(\"Top expert used:\", expert_used)\n",
    "    print(\"Gating probabilities:\", np.round(gating_distribution, 3))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_array = np.array(image)\n",
    "    mean_colors = image_array.mean(axis=(0, 1))\n",
    "    color_names = ['red', 'green', 'blue']\n",
    "    dominant_color = color_names[np.argmax(mean_colors)]\n",
    "    return f\"Dominant color is {dominant_color}.\"\n",
    "\n",
    "def multimodal_response(model, input_text=None, image_path=None):\n",
    "    result = \"\"\n",
    "    if input_text:\n",
    "        # Use your generate_response logic for text\n",
    "        response, exp_used, gating_distribution = generate_response(model, input_text, max_len=30)\n",
    "        result += f\"Text response: {response}\\nTop expert used: {exp_used}\\n\"\n",
    "    if image_path:\n",
    "        # Classify the image\n",
    "        image_result = classify_image(image_path)\n",
    "        result += f\"Image analysis: {image_result}\\n\"\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "print(multimodal_response(model, input_text=\"what toppings do you have?\", image_path=\"sample_image.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Softmax, LSTM, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# ----------- MNIST DIGIT CLASSIFIER -----------\n",
    "\n",
    "class DigitClassifier(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(32, (3,3), activation='relu')\n",
    "        self.pool1 = MaxPooling2D((2,2))\n",
    "        self.conv2 = Conv2D(64, (3,3), activation='relu')\n",
    "        self.pool2 = MaxPooling2D((2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(64, activation='relu')\n",
    "        self.out = Dense(10)  # digits 0-9 logits\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return self.out(x)\n",
    "\n",
    "def prepare_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# Train or load a pretrained digit classifier\n",
    "digit_classifier = DigitClassifier()\n",
    "(x_train, y_train), (x_test, y_test) = prepare_mnist_data()\n",
    "digit_classifier.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "print(\"Training digit classifier (this may take a bit)...\")\n",
    "digit_classifier.fit(x_train, y_train, epochs=3, batch_size=128, validation_split=0.1)\n",
    "\n",
    "# ----------- YOUR MOE TEXT GENERATION MODEL (copy your existing) -----------\n",
    "\n",
    "# (Paste your current MoEResponseGenerator, Expert, GatingNetwork, encode_sentence, sample_from_logits, etc. here)\n",
    "# For brevity, let's assume it's loaded as `moe_model`\n",
    "\n",
    "# For this example, just a placeholder generate_response:\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    # Use your real model's generation logic here\n",
    "    return f\"Simulated response to: {input_text}\", 1, np.array([0.1, 0.7, 0.1, 0.1])\n",
    "\n",
    "# ----------- MNIST INCREMENT LOGIC -----------\n",
    "\n",
    "def get_incremented_digit_image(input_image, x_dataset, y_dataset):\n",
    "    # Normalize pixel values and ensure float32\n",
    "    img = input_image.astype('float32')\n",
    "    if img.max() > 1.0:\n",
    "        img /= 255.0\n",
    "\n",
    "    # Add channel dimension if missing\n",
    "    if len(img.shape) == 2:   # grayscale image shape (28,28)\n",
    "        img = np.expand_dims(img, axis=-1)  # become (28,28,1)\n",
    "\n",
    "    # Add batch dimension for model input\n",
    "    img = np.expand_dims(img, axis=0)  # become (1,28,28,1)\n",
    "\n",
    "    logits = digit_classifier(img)\n",
    "    pred_digit = tf.argmax(logits, axis=1).numpy()[0]\n",
    "\n",
    "    inc_digit = 0 if pred_digit == 9 else pred_digit + 1\n",
    "    idx = np.where(y_dataset == inc_digit)[0][0]\n",
    "    inc_image = x_dataset[idx][:, :, 0]  # remove channel for display\n",
    "\n",
    "    return pred_digit, inc_digit, inc_image\n",
    "\n",
    "\n",
    "# ----------- MULTIMODAL HANDLER -----------\n",
    "\n",
    "def multimodal_handler(input_text=None, input_image=None):\n",
    "    if input_text:\n",
    "        response, expert, gating = generate_response(None, input_text)\n",
    "        print(f\"Text Input: {input_text}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Top expert used: {expert}\")\n",
    "        print(f\"Gating probabilities: {gating}\")\n",
    "    if input_image is not None:\n",
    "        pred_digit, inc_digit, inc_image = get_incremented_digit_image(input_image, x_test, np.argmax(y_test, axis=1))\n",
    "        print(f\"Input Digit: {pred_digit}, Incremented Digit: {inc_digit}\")\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title(f\"Input: {pred_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(inc_image, cmap='gray')\n",
    "        plt.title(f\"Output: {inc_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# ----------- DEMO -----------\n",
    "\n",
    "# Text-only\n",
    "multimodal_handler(input_text=\"I need help with my order.\")\n",
    "\n",
    "# Image-only\n",
    "multimodal_handler(input_image=x_test[5])\n",
    "\n",
    "# Both modalities\n",
    "multimodal_handler(input_text=\"What digit is this?\", input_image=x_test[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_3.10_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
