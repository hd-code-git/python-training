{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.9639 - train_accuracy: 0.6071\n",
      "Epoch 2/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.6707 - train_accuracy: 0.6786\n",
      "Epoch 3/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.9289 - train_accuracy: 0.7143\n",
      "Epoch 4/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.8789 - train_accuracy: 0.8929\n",
      "Epoch 5/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 6/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 7/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 8/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 9/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 10/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 11/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 12/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 13/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 14/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 15/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 16/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 17/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 18/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 19/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 20/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 21/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 22/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 23/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 24/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8789 - train_accuracy: 0.9286\n",
      "Epoch 25/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.6836 - train_accuracy: 0.9286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x8277ae5f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "sound_vocab = {\"<pad>\": 0, \"cat\": 1, \"dog\": 2, \"meow\": 3, \"bark\": 4}\n",
    "inv_sound_vocab = {v: k for k, v in sound_vocab.items()}\n",
    "animal_vocab = {\"<pad>\": 0, \"cat\": 1, \"dog\": 2}\n",
    "inv_animal_vocab = {v: k for k, v in animal_vocab.items()}\n",
    "\n",
    "dummy_text = \"<empty>\"\n",
    "dummy_image = np.zeros((224,224,3), dtype=np.float32)\n",
    "\n",
    "# Text expert\n",
    "class HubEmbeddingLayer(Layer):\n",
    "    def __init__(self, hub_url=\"https://tfhub.dev/google/universal-sentence-encoder/4\"):\n",
    "        super().__init__()\n",
    "        self.hub_layer = hub.KerasLayer(hub_url, trainable=True)\n",
    "    def call(self, inputs):\n",
    "        return self.hub_layer(inputs)\n",
    "\n",
    "def build_text_expert():\n",
    "    inp = Input(shape=(), dtype=tf.string)\n",
    "    x = HubEmbeddingLayer()(inp)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(len(sound_vocab))(x)\n",
    "    return Model(inp, out, name=\"TextExpert\")\n",
    "\n",
    "# Image expert\n",
    "def build_image_expert():\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=(224,224,3), include_top=False,\n",
    "                                                   weights=\"imagenet\", pooling=\"avg\")\n",
    "    base_model.trainable = False\n",
    "    inp = Input(shape=(224,224,3))\n",
    "    x = base_model(inp)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    out = layers.Dense(len(animal_vocab))(x)\n",
    "    return Model(inp, out, name=\"ImageExpert\")\n",
    "\n",
    "# Deterministic gating layer selecting expert by presence flags\n",
    "class GateLayer(Layer):\n",
    "    def call(self, inputs):\n",
    "        text_present, image_present = inputs  # shape (batch,1)\n",
    "        gate_text = 1.0 - image_present  # text expert if no image\n",
    "        gate_image = image_present       # image expert if image present\n",
    "        gates = tf.concat([gate_text, gate_image], axis=1)\n",
    "        return gates\n",
    "\n",
    "def gating_entropy_loss(gate_weights):\n",
    "    return -tf.reduce_mean(tf.reduce_sum(gate_weights * tf.math.log(gate_weights + 1e-10), axis=1))\n",
    "\n",
    "class PadLayer(Layer):\n",
    "    def __init__(self, pad_width):\n",
    "        super().__init__()\n",
    "        self.pad_width = pad_width\n",
    "    def call(self, inputs):\n",
    "        neg_inf = tf.fill([tf.shape(inputs)[0], self.pad_width], -1e9)\n",
    "        return tf.concat([inputs, neg_inf], axis=1)\n",
    "\n",
    "class SoftmaxWithTemp(Layer):\n",
    "    def __init__(self, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.softmax(inputs / self.temperature)\n",
    "\n",
    "def load_image_dataset():\n",
    "    ds, info = tfds.load('cats_vs_dogs', split='train', with_info=True)\n",
    "    cat_images, dog_images = [], []\n",
    "    for ex in ds.as_numpy_iterator():\n",
    "        label = ex['label']\n",
    "        image = ex['image']\n",
    "        image = tf.image.resize(image, (224,224)).numpy() / 255.0\n",
    "        if label == 0 and len(cat_images) < 10:\n",
    "            cat_images.append(image)\n",
    "        elif label == 1 and len(dog_images) < 10:\n",
    "            dog_images.append(image)\n",
    "        if len(cat_images) == 10 and len(dog_images) == 10:\n",
    "            break\n",
    "    images = np.concatenate([np.array(cat_images), np.array(dog_images)], axis=0)\n",
    "    labels = np.array([animal_vocab['cat']] * 10 + [animal_vocab['dog']] * 10, dtype=np.int32)\n",
    "    text_inputs = np.array([dummy_text] * len(images))\n",
    "    text_present = np.zeros((len(images),1), dtype=np.float32)\n",
    "    image_present = np.ones((len(images),1), dtype=np.float32)\n",
    "    return tf.data.Dataset.from_tensor_slices(((text_inputs, images, text_present, image_present), labels))\n",
    "\n",
    "def load_text_dataset():\n",
    "    texts = [\"cat\", \"dog\", \"cat\", \"dog\", \"cat\", \"dog\", \"cat\", \"dog\"]\n",
    "    sound_labels = np.array(\n",
    "        [sound_vocab[\"meow\"], sound_vocab[\"bark\"], sound_vocab[\"meow\"], sound_vocab[\"bark\"],\n",
    "         sound_vocab[\"meow\"], sound_vocab[\"bark\"], sound_vocab[\"meow\"], sound_vocab[\"bark\"]],\n",
    "        dtype=np.int32)\n",
    "    text_present = np.ones((len(texts),1), dtype=np.float32)\n",
    "    image_present = np.zeros((len(texts),1), dtype=np.float32)\n",
    "    dummy_images = np.zeros((len(texts), 224, 224, 3), dtype=np.float32)\n",
    "    return tf.data.Dataset.from_tensor_slices(((texts, dummy_images, text_present, image_present), sound_labels))\n",
    "\n",
    "def prepare_combined_dataset():\n",
    "    image_ds = load_image_dataset()\n",
    "    text_ds = load_text_dataset()\n",
    "    combined = image_ds.concatenate(text_ds)\n",
    "    combined = combined.shuffle(32).batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "    return combined\n",
    "\n",
    "\n",
    "class MoeModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_expert = build_text_expert()\n",
    "        self.image_expert = build_image_expert()\n",
    "        self.gate = GateLayer()\n",
    "        self.pad_layer = PadLayer(len(sound_vocab) - len(animal_vocab))\n",
    "        self.softmax_temp = SoftmaxWithTemp(0.7)\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        self.lambda_image_aux = 0.8\n",
    "        self.lambda_entropy = 0.01\n",
    "        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        text_input, image_input, text_present, image_present = inputs\n",
    "        gate_weights = self.gate([text_present, image_present])\n",
    "        text_logits = self.text_expert(text_input, training=training)\n",
    "        image_logits = self.image_expert(image_input, training=training)\n",
    "        image_logits_padded = self.pad_layer(image_logits)\n",
    "\n",
    "        text_logits_norm = self.softmax_temp(text_logits)\n",
    "        image_logits_norm = self.softmax_temp(image_logits_padded)\n",
    "\n",
    "        combined_logits = gate_weights[:, 0:1] * text_logits_norm + gate_weights[:, 1:2] * image_logits_norm\n",
    "        outputs = tf.nn.softmax(combined_logits)\n",
    "        return outputs, gate_weights, text_logits, image_logits\n",
    "\n",
    "    def train_step(self, data):\n",
    "        (text_input, image_input, text_present, image_present), labels = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs, gate_weights, text_logits, image_logits = self((text_input, image_input, text_present, image_present), training=True)\n",
    "\n",
    "            image_present_flat = tf.reshape(image_present, [-1])\n",
    "            mask_image = tf.equal(image_present_flat, 1.0)\n",
    "            mask_text = tf.logical_not(mask_image)\n",
    "\n",
    "            text_labels = tf.boolean_mask(labels, mask_text)\n",
    "            text_logits_masked = tf.boolean_mask(text_logits, mask_text)\n",
    "            text_logits_masked = text_logits_masked[:, :len(sound_vocab)]\n",
    "\n",
    "            image_labels = tf.boolean_mask(labels, mask_image)\n",
    "            image_logits_masked = tf.boolean_mask(image_logits, mask_image)\n",
    "            image_logits_masked = image_logits_masked[:, :len(animal_vocab)]\n",
    "\n",
    "            text_loss = tf.cond(tf.greater(tf.size(text_labels), 0),\n",
    "                                lambda: self.loss_fn(text_labels, text_logits_masked),\n",
    "                                lambda: 0.0)\n",
    "\n",
    "            image_loss = tf.cond(tf.greater(tf.size(image_labels), 0),\n",
    "                                lambda: self.loss_fn(image_labels, image_logits_masked),\n",
    "                                lambda: 0.0)\n",
    "\n",
    "            entropy_loss = gating_entropy_loss(gate_weights)\n",
    "            total_loss = text_loss + self.lambda_image_aux * image_loss + self.lambda_entropy * entropy_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.train_accuracy.update_state(labels, outputs)\n",
    "        return {\"loss\": total_loss, \"train_accuracy\": self.train_accuracy.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (text_input, image_input, text_present, image_present), labels = data\n",
    "        outputs, _, _, _ = self((text_input, image_input, text_present, image_present), training=False)\n",
    "        loss = self.loss_fn(labels, outputs)\n",
    "        self.val_accuracy.update_state(labels, outputs)\n",
    "        return {\"loss\": loss, \"val_accuracy\": self.val_accuracy.result()}\n",
    "\n",
    "moe_model = MoeModel()\n",
    "moe_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "train_dataset = prepare_combined_dataset()\n",
    "\n",
    "moe_model.fit(train_dataset, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: text input 'dog'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450ms/step\n",
      "Softmax output vector: [0.19037393 0.18719237 0.19139642 0.19163263 0.23940468]\n",
      "Gate weights: [1. 0.]\n",
      "Input: text='dog', image_present=0.0\n",
      "Prediction: bark\n",
      "--------------------------------------------------\n",
      "Test: text input 'canine'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Softmax output vector: [0.19367616 0.19007787 0.1937225  0.19740176 0.22512168]\n",
      "Gate weights: [1. 0.]\n",
      "Input: text='canine', image_present=0.0\n",
      "Prediction: bark\n",
      "--------------------------------------------------\n",
      "Test: text input 'cat'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Softmax output vector: [0.18833591 0.19395396 0.19642694 0.23239923 0.18888393]\n",
      "Gate weights: [1. 0.]\n",
      "Input: text='cat', image_present=0.0\n",
      "Prediction: meow\n",
      "--------------------------------------------------\n",
      "Test: text input 'feline'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Softmax output vector: [0.19249156 0.19474043 0.19746724 0.2214343  0.19386642]\n",
      "Gate weights: [1. 0.]\n",
      "Input: text='feline', image_present=0.0\n",
      "Prediction: meow\n",
      "--------------------------------------------------\n",
      "Test: image of cat\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Softmax output vector: [0.15038177 0.39310712 0.15599376 0.15025869 0.15025869]\n",
      "Gate weights: [0. 1.]\n",
      "Input: text='<empty>', image_present=1.0\n",
      "Prediction: cat\n",
      "--------------------------------------------------\n",
      "Test: image of dog\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Softmax output vector: [0.15778488 0.21954952 0.30738774 0.15763886 0.15763886]\n",
      "Gate weights: [0. 1.]\n",
      "Input: text='<empty>', image_present=1.0\n",
      "Prediction: dog\n",
      "--------------------------------------------------\n",
      "Test: text 'cat' and image cat\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Softmax output vector: [0.15038177 0.39310712 0.15599376 0.15025869 0.15025869]\n",
      "Gate weights: [0. 1.]\n",
      "Input: text='cat', image_present=1.0\n",
      "Prediction: cat\n",
      "--------------------------------------------------\n",
      "Test: text 'cat' and image dog\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Softmax output vector: [0.15778488 0.21954952 0.30738774 0.15763886 0.15763886]\n",
      "Gate weights: [0. 1.]\n",
      "Input: text='cat', image_present=1.0\n",
      "Prediction: dog\n",
      "--------------------------------------------------\n",
      "Test: text 'dog' and image dog\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Softmax output vector: [0.15778488 0.21954952 0.30738774 0.15763886 0.15763886]\n",
      "Gate weights: [0. 1.]\n",
      "Input: text='dog', image_present=1.0\n",
      "Prediction: dog\n",
      "--------------------------------------------------\n",
      "Test: text 'dog' and image cat\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Softmax output vector: [0.15038177 0.39310712 0.15599376 0.15025869 0.15025869]\n",
      "Gate weights: [0. 1.]\n",
      "Input: text='dog', image_present=1.0\n",
      "Prediction: cat\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_with_moe(text_input=None, image_input=None):\n",
    "    if text_input is None or text_input == \"\":\n",
    "        text_input = dummy_text\n",
    "    if image_input is None:\n",
    "        image_input = dummy_image\n",
    "\n",
    "    if image_input.ndim == 3 and image_input.shape[2] == 1:\n",
    "        image_input = np.concatenate([image_input]*3, axis=2)\n",
    "\n",
    "    if image_input.shape != (224,224,3):\n",
    "        image_input = tf.image.resize(image_input, (224,224)).numpy()\n",
    "\n",
    "    text_present_f = 0.0 if text_input == dummy_text else 1.0\n",
    "    image_present_f = 0.0 if np.all(image_input == 0) else 1.0\n",
    "\n",
    "    preds, gate_weights, text_logits, image_logits = moe_model.predict([\n",
    "        tf.constant([text_input]),\n",
    "        np.array([image_input]),\n",
    "        np.array([[text_present_f]], dtype=np.float32),\n",
    "        np.array([[image_present_f]], dtype=np.float32)\n",
    "    ])\n",
    "\n",
    "    if image_present_f == 1.0:\n",
    "        image_probs = preds[0][:len(animal_vocab)]\n",
    "        pred_id = np.argmax(image_probs)\n",
    "        pred_label = inv_animal_vocab.get(pred_id, \"unknown\")\n",
    "    else:\n",
    "        pred_id = np.argmax(preds[0])\n",
    "        pred_label = inv_sound_vocab.get(pred_id, \"unknown\")\n",
    "\n",
    "    print(f\"Softmax output vector: {preds[0]}\")\n",
    "    print(f\"Gate weights: {gate_weights[0]}\")\n",
    "    print(f\"Input: text='{text_input}', image_present={image_present_f}\")\n",
    "    print(f\"Prediction: {pred_label}\\n{'-'*50}\")\n",
    "    return pred_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example Tests\n",
    "\n",
    "print(\"Test: text input 'dog'\")\n",
    "predict_with_moe(text_input=\"dog\")\n",
    "\n",
    "print(\"Test: text input 'canine'\")\n",
    "predict_with_moe(text_input=\"canine\")\n",
    "\n",
    "print(\"Test: text input 'cat'\")\n",
    "predict_with_moe(text_input=\"cat\")\n",
    "\n",
    "print(\"Test: text input 'feline'\")\n",
    "predict_with_moe(text_input=\"feline\")\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Example real image test (you may replace URL with your own image links)\n",
    "def load_image(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "    img = img.resize((224,224))\n",
    "    img_array = np.array(img) / 255.0\n",
    "    return img_array\n",
    "\n",
    "cat_img_url = \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131\"\n",
    "dog_img_url = \"https://images.unsplash.com/photo-1517423440428-a5a00ad493e8\"\n",
    "\n",
    "print(\"Test: image of cat\")\n",
    "predict_with_moe(image_input=load_image(cat_img_url))\n",
    "\n",
    "print(\"Test: image of dog\")\n",
    "predict_with_moe(image_input=load_image(dog_img_url))\n",
    "\n",
    "print(\"Test: text 'cat' and image cat\")\n",
    "predict_with_moe(text_input=\"cat\", image_input=load_image(cat_img_url))\n",
    "\n",
    "print(\"Test: text 'cat' and image dog\")\n",
    "predict_with_moe(text_input=\"cat\", image_input=load_image(dog_img_url))\n",
    "\n",
    "print(\"Test: text 'dog' and image dog\")\n",
    "predict_with_moe(text_input=\"dog\", image_input=load_image(dog_img_url))\n",
    "\n",
    "print(\"Test: text 'dog' and image cat\")\n",
    "predict_with_moe(text_input=\"dog\", image_input=load_image(cat_img_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Softmax, LSTM, Conv2D, Flatten, MaxPooling2D, LayerNormalization, Dropout, Input, Conv2DTranspose, Reshape, Add, Lambda, BatchNormalization, add, Activation\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# MoE Text Generation Model Components\n",
    "# ---------------------------------------------\n",
    "\n",
    "class Expert(Model):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.dense2 = Dense(d_model)\n",
    "    def call(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "class GatingNetwork(Model):\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super().__init__()\n",
    "        self.dense = Dense(num_experts)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "    def call(self, x):\n",
    "        logits = self.dense(x)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "class MoEResponseGenerator(Model):\n",
    "    def __init__(self, vocab_size, d_model, num_experts, max_resp_len, lstm_units=128):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = [Expert(d_model) for _ in range(num_experts)]\n",
    "        self.gating_net = GatingNetwork(d_model, num_experts)\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        self.to_h = Dense(lstm_units)\n",
    "        self.to_c = Dense(lstm_units)\n",
    "        self.output_layer = Dense(vocab_size)\n",
    "        self.max_resp_len = max_resp_len\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_seq, resp_in_seq = inputs\n",
    "        enc_emb = self.embedding(input_seq)\n",
    "        pooled = self.global_pool(enc_emb)\n",
    "\n",
    "        gating_probs = self.gating_net(pooled)\n",
    "        expert_outs = tf.stack([expert(pooled) for expert in self.experts], axis=1)\n",
    "        gated_rep = tf.reduce_sum(tf.expand_dims(gating_probs, 2) * expert_outs, axis=1)\n",
    "\n",
    "        h_state = self.to_h(gated_rep)\n",
    "        c_state = self.to_c(gated_rep)\n",
    "\n",
    "        resp_emb = self.embedding(resp_in_seq)\n",
    "        lstm_out, _, _ = self.lstm(resp_emb, initial_state=[h_state, c_state])\n",
    "\n",
    "        logits = self.output_layer(lstm_out)\n",
    "        return logits, gating_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Utility functions for text encoding and generation\n",
    "# ---------------------------------------------\n",
    "\n",
    "# (Your train_data list is assumed defined here - omitted for brevity, but same as your list)\n",
    "\n",
    "# Build vocabulary based on train_data (input + responses)\n",
    "train_data = [\n",
    "    (\"hello there\", \"hi, how can I help\"),\n",
    "    (\"hi\", \"hello, what can I do\"),\n",
    "    (\"goodbye\", \"goodbye, have a nice day\"),\n",
    "    (\"see you later\", \"see you soon, goodbye\"),\n",
    "    (\"I want to order pizza\", \"sure, what toppings do you want\"),\n",
    "    (\"can I get a burger\", \"what size burger would you like\"),\n",
    "    (\"what is the weather\", \"the weather today is sunny\"),\n",
    "    (\"is it raining\", \"no rain expected today\"),\n",
    "    (\"hey, I want some pasta\", \"what kind of pasta would you prefer\"),\n",
    "    (\"do you have vegetarian options?\", \"yes, we have several vegetarian dishes\"),\n",
    "    (\"good morning\", \"good morning, how may I assist you?\"),\n",
    "    (\"bye\", \"take care, see you later\"),\n",
    "    (\"will it be hot today?\", \"expect warm temperatures all day\"),\n",
    "    (\"can I order a salad?\", \"what dressing would you like on your salad?\"),\n",
    "    (\"thanks, goodbye\", \"you're welcome, goodbye!\"),\n",
    "    (\"tell me the forecast\", \"the forecast shows clear skies\"),\n",
    "    (\"what's your name?\", \"i am your assistant, here to help\"),\n",
    "    (\"can I have a coffee?\", \"sure, would you like it black or with milk?\"),\n",
    "    (\"thank you for the help\", \"happy to assist you anytime\"),\n",
    "    (\"are you open today?\", \"yes, we are open from 9 am to 9 pm\"),\n",
    "    (\"could you help me with my order\", \"of course, what would you like to order\"),\n",
    "    (\"are there any gluten free options\", \"yes, we have several gluten free dishes available\"),\n",
    "    (\"what are today's specials\", \"today's special is grilled salmon with vegetables\"),\n",
    "    (\"how late are you open\", \"we are open until 10 pm tonight\"),\n",
    "    (\"can you recommend a dessert\", \"our chocolate lava cake is very popular\"),\n",
    "    (\"I need to change my order\", \"sure, what changes would you like to make\"),\n",
    "    (\"do you deliver\", \"yes, we deliver within a 5 mile radius\"),\n",
    "    (\"what payment methods do you accept\", \"we accept cash, credit cards, and mobile payments\"),\n",
    "    (\"is there a parking facility\", \"yes, free parking is available behind the restaurant\"),\n",
    "    (\"thank you very much\", \"you're welcome, happy to help\"),\n",
    "    (\"I have a food allergy\", \"please let us know your allergy, and we will accommodate\"),\n",
    "    (\"can I book a table\", \"yes, for how many people and what time\"),\n",
    "    (\"what's your restaurant address\", \"we are located at 123 Main Street\"),\n",
    "    (\"do you have vegan meals\", \"yes, we offer delicious vegan options\"),\n",
    "    (\"can I get nutritional information\", \"nutritional info is available on our website\"),\n",
    "    (\"how long is the wait time\", \"usually about 15 minutes during peak hours\"),\n",
    "    (\"do you have a kids menu\", \"yes, we have a special menu for children\"),\n",
    "    (\"can I cancel my order\", \"please provide your order number to cancel\"),\n",
    "    (\"what are your opening hours\", \"we are open from 9 am to 10 pm daily\"),\n",
    "    (\"is takeout available\", \"yes, you can order takeout anytime during opening hours\"),        \n",
    "]\n",
    "\n",
    "all_texts = [t[0] + \" \" + t[1] for t in train_data]\n",
    "all_words = set(word for sentence in all_texts for word in sentence.lower().split())\n",
    "word2idx = {w: i + 1 for i, w in enumerate(sorted(all_words))}\n",
    "idx2word = np.array(['<pad>'] + sorted(all_words))\n",
    "vocab_size = len(idx2word)\n",
    "max_input_len = 6\n",
    "max_resp_len = 8\n",
    "\n",
    "def encode_sentence(sent, max_len):\n",
    "    words = sent.lower().split()\n",
    "    seq = [word2idx.get(w, 0) for w in words]\n",
    "    seq = seq[:max_len] + [0] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0, top_k=5):\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        values, _ = tf.math.top_k(logits, k=top_k)\n",
    "        min_values = values[:, -1, None]\n",
    "        logits = tf.where(\n",
    "            logits < min_values,\n",
    "            tf.fill(tf.shape(logits), float('-inf')),\n",
    "            logits,\n",
    "        )\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    next_token = tf.random.categorical(tf.math.log(probabilities), num_samples=1)\n",
    "    return tf.squeeze(next_token, axis=-1).numpy()\n",
    "\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    input_seq = np.array([encode_sentence(input_text, max_input_len)])\n",
    "    response_seq = np.zeros((1, max_resp_len), dtype=np.int32)\n",
    "    generated_tokens = []\n",
    "    gating_probs = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        logits, gating_probs = model((input_seq, response_seq), training=False)\n",
    "        logits_step = logits[:, i % max_resp_len, :]\n",
    "        next_token = sample_from_logits(logits_step, temperature=temperature, top_k=top_k)[0]\n",
    "        if next_token == 0:\n",
    "            break\n",
    "        generated_tokens.append(idx2word[next_token])\n",
    "        if i + 1 < max_resp_len:\n",
    "            response_seq[0, i] = next_token\n",
    "        else:\n",
    "            response_seq = np.roll(response_seq, -1, axis=1)\n",
    "            response_seq[0, -1] = next_token\n",
    "\n",
    "    top_expert = np.argmax(gating_probs[0].numpy()) if gating_probs is not None else -1\n",
    "    return \" \".join(generated_tokens), top_expert, gating_probs[0].numpy()\n",
    "\n",
    "# Build training dataset\n",
    "X_input = np.array([encode_sentence(t[0], max_input_len) for t in train_data])\n",
    "X_resp_in = np.array([encode_sentence(t[1], max_resp_len) for t in train_data])\n",
    "X_resp_out = np.array([encode_sentence(t[1], max_resp_len)[1:] + [0] for t in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# MNIST Digit Classifier Components\n",
    "# ---------------------------------------------\n",
    "\n",
    "class DigitClassifier(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(32, (3,3), activation='relu')\n",
    "        self.pool1 = MaxPooling2D((2,2))\n",
    "        self.conv2 = Conv2D(64, (3,3), activation='relu')\n",
    "        self.pool2 = MaxPooling2D((2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(64, activation='relu')\n",
    "        self.out = Dense(10)  # digits 0-9 logits\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return self.out(x)\n",
    "\n",
    "def prepare_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Training setup and training loops\n",
    "# ---------------------------------------------\n",
    "\n",
    "digit_classifier = DigitClassifier()\n",
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = prepare_mnist_data()\n",
    "digit_classifier.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "print(\"Training MNIST digit classifier...\")\n",
    "digit_classifier.fit(x_train_mnist, y_train_mnist, epochs=3, batch_size=128, validation_split=0.1)\n",
    "\n",
    "d_model = 32\n",
    "num_experts = 5\n",
    "lstm_units = 128\n",
    "max_epochs = 300\n",
    "batch_size = 2\n",
    "\n",
    "moe_model = MoEResponseGenerator(vocab_size, d_model, num_experts, max_resp_len, lstm_units)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((X_input, X_resp_in), X_resp_out)).shuffle(50).batch(batch_size)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, _ = moe_model(inputs, training=True)\n",
    "        loss = loss_fn(labels, logits)\n",
    "    grads = tape.gradient(loss, moe_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, moe_model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "print(\"Training MoE text generation model...\")\n",
    "for epoch in range(max_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in train_dataset:\n",
    "        loss = train_step(batch[0], batch[1])\n",
    "        total_loss += loss.numpy()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{max_epochs}: Loss = {total_loss / len(train_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "# Load and normalize MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Prepare target images for increment digits\n",
    "train_target_images = np.zeros_like(x_train)\n",
    "for idx in range(len(x_train)):\n",
    "    inc_digit = (y_train[idx] + 1) % 10\n",
    "    candidates = np.where(y_train == inc_digit)[0]\n",
    "    chosen_idx = np.random.choice(candidates)\n",
    "    train_target_images[idx] = x_train[chosen_idx]\n",
    "\n",
    "test_target_images = np.zeros_like(x_test)\n",
    "for idx in range(len(x_test)):\n",
    "    inc_digit = (y_test[idx] + 1) % 10\n",
    "    candidates = np.where(y_test == inc_digit)[0]\n",
    "    chosen_idx = np.random.choice(candidates)\n",
    "    test_target_images[idx] = x_test[chosen_idx]\n",
    "\n",
    "# Residual block\n",
    "def residual_block(x, filters, kernel_size=3):\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = add([shortcut, x])\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "# Build model with residual blocks\n",
    "def build_improved_increment_model():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    x = Conv2D(64, 3, strides=2, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x = Conv2D(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = residual_block(x, 128)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(7*7*128, activation=\"relu\")(x)\n",
    "    x = Reshape((7,7,128))(x)\n",
    "    x = Conv2DTranspose(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2DTranspose(64, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Conv2D(1, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "increment_model = build_improved_increment_model()\n",
    "\n",
    "# Initialize VGG16 model for perceptual loss - with input shape 32x32x3 (minimum required)\n",
    "vgg = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "vgg.trainable = False\n",
    "feature_extractor = Model(vgg.input, vgg.get_layer(\"block3_conv3\").output)\n",
    "\n",
    "# Preprocessing function to resize and replicate grayscale channel for VGG16\n",
    "def preprocess_for_vgg(x):\n",
    "    x_resized = tf.image.resize(x, [32, 32])\n",
    "    x_rgb = tf.image.grayscale_to_rgb(x_resized)\n",
    "    x_preprocessed = preprocess_input(x_rgb * 255.0)\n",
    "    return x_preprocessed\n",
    "\n",
    "# Perceptual loss combining MSE and VGG feature loss\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    y_true_pp = preprocess_for_vgg(y_true)\n",
    "    y_pred_pp = preprocess_for_vgg(y_pred)\n",
    "    f_true = feature_extractor(y_true_pp)\n",
    "    f_pred = feature_extractor(y_pred_pp)\n",
    "    return tf.reduce_mean(tf.square(f_true - f_pred))\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    pl = perceptual_loss(y_true, y_pred)\n",
    "    return mse + 0.1 * pl\n",
    "\n",
    "increment_model.compile(optimizer='adam', loss=combined_loss)\n",
    "\n",
    "# Train with learning rate scheduler callback\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "increment_model.fit(\n",
    "    x_train,\n",
    "    train_target_images,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_test, test_target_images),\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n",
    "\n",
    "# Inference helper\n",
    "def get_incremented_digit_image(input_image):\n",
    "    img = input_image.astype(\"float32\") / 255.0\n",
    "    if img.ndim == 2:\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    generated_img = increment_model.predict(img)\n",
    "    return generated_img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of test results for a random sample\n",
    "# Visualization test\n",
    "idx = np.random.randint(len(x_test))\n",
    "input_img = x_test[idx]\n",
    "original_digit = y_test[idx]\n",
    "inc_digit = (original_digit + 1) % 10\n",
    "generated_img = get_incremented_digit_image(input_img)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(f\"Input: {original_digit}\")\n",
    "plt.imshow(input_img[:,:,0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(f\"Generated: {inc_digit}\")\n",
    "plt.imshow(generated_img[:,:,0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(f\"Expected: {inc_digit}\")\n",
    "expected_img = x_test[np.where(y_test == inc_digit)[0][0]]\n",
    "plt.imshow(expected_img[:,:,0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_handler(input_text=None, input_image=None):\n",
    "    if input_text:\n",
    "        response, expert, gating_probs = generate_response(moe_model, input_text)\n",
    "        print(f\"Text Input: {input_text}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Top expert used: {expert}\")\n",
    "        print(f\"Gating probs: {np.round(gating_probs, 3)}\")\n",
    "    if input_image is not None:\n",
    "        pred_digit, inc_digit, inc_image = get_incremented_digit_image_generative(input_image)\n",
    "        print(f\"Image input digit: {pred_digit}, incremented output digit: {inc_digit}\")\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title(f\"Input: {pred_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(inc_image, cmap='gray')\n",
    "        plt.title(f\"Output: {inc_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Demo code to test multimodal system\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Text only\n",
    "multimodal_handler(input_text=\"hello, how are you?\")\n",
    "\n",
    "# Image only (from MNIST test)\n",
    "multimodal_handler(input_image=x_test_mnist[15])\n",
    "\n",
    "# Both\n",
    "multimodal_handler(input_text=\"What number is this?\", input_image=x_test_mnist[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_examples = [\n",
    "    \"hello\",\n",
    "    \"hi there\",\n",
    "    \"good morning\",\n",
    "    \"hey, how are you?\",\n",
    "    \"what's up?\"\n",
    "]\n",
    "\n",
    "goodbye_examples = [\n",
    "    \"goodbye\",\n",
    "    \"see you later\",\n",
    "    \"bye for now\",\n",
    "    \"talk to you soon\",\n",
    "    \"have a great day\"\n",
    "]\n",
    "\n",
    "order_food_examples = [\n",
    "    \"I want to order a pizza\",\n",
    "    \"can I get a burger please?\",\n",
    "    \"what sides do you have?\",\n",
    "    \"I'd like a vegetarian pasta\",\n",
    "    \"do you have gluten free options?\"\n",
    "]\n",
    "\n",
    "weather_examples = [\n",
    "    \"what's the weather today?\",\n",
    "    \"will it rain tomorrow?\",\n",
    "    \"is it sunny outside?\",\n",
    "    \"what's the forecast for this week?\",\n",
    "    \"do I need an umbrella today?\"\n",
    "]\n",
    "\n",
    "miscellaneous_examples = [\n",
    "    \"what's your name?\",\n",
    "    \"can you tell me a joke?\",\n",
    "    \"how do I say goodbye politely?\",\n",
    "    \"are you open on weekends?\",\n",
    "    \"what time do you close?\"\n",
    "]\n",
    "\n",
    "inference_prompts = greeting_examples + goodbye_examples + order_food_examples + weather_examples + miscellaneous_examples\n",
    "for input_text in inference_prompts:\n",
    "    multimodal_handler(input_text)\n",
    "\n",
    "    # print(\"Input:\", input_text)\n",
    "    # print(\"Generated response:\", response)\n",
    "    # print(\"Top expert used:\", expert_used)\n",
    "    # print(\"Gating probabilities:\", np.round(gating_distribution, 3))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (input, response)\n",
    "train_data = [\n",
    "    (\"hello there\", \"hi, how can I help\"),\n",
    "    (\"hi\", \"hello, what can I do\"),\n",
    "    (\"goodbye\", \"goodbye, have a nice day\"),\n",
    "    (\"see you later\", \"see you soon, goodbye\"),\n",
    "    (\"I want to order pizza\", \"sure, what toppings do you want\"),\n",
    "    (\"can I get a burger\", \"what size burger would you like\"),\n",
    "    (\"what is the weather\", \"the weather today is sunny\"),\n",
    "    (\"is it raining\", \"no rain expected today\"),\n",
    "    (\"hey, I want some pasta\", \"what kind of pasta would you prefer\"),\n",
    "    (\"do you have vegetarian options?\", \"yes, we have several vegetarian dishes\"),\n",
    "    (\"good morning\", \"good morning, how may I assist you?\"),\n",
    "    (\"bye\", \"take care, see you later\"),\n",
    "    (\"will it be hot today?\", \"expect warm temperatures all day\"),\n",
    "    (\"can I order a salad?\", \"what dressing would you like on your salad?\"),\n",
    "    (\"thanks, goodbye\", \"you're welcome, goodbye!\"),\n",
    "    (\"tell me the forecast\", \"the forecast shows clear skies\"),\n",
    "    (\"what's your name?\", \"i am your assistant, here to help\"),\n",
    "    (\"can I have a coffee?\", \"sure, would you like it black or with milk?\"),\n",
    "    (\"thank you for the help\", \"happy to assist you anytime\"),\n",
    "    (\"are you open today?\", \"yes, we are open from 9 am to 9 pm\"),\n",
    "    (\"could you help me with my order\", \"of course, what would you like to order\"),\n",
    "    (\"are there any gluten free options\", \"yes, we have several gluten free dishes available\"),\n",
    "    (\"what are today's specials\", \"today's special is grilled salmon with vegetables\"),\n",
    "    (\"how late are you open\", \"we are open until 10 pm tonight\"),\n",
    "    (\"can you recommend a dessert\", \"our chocolate lava cake is very popular\"),\n",
    "    (\"I need to change my order\", \"sure, what changes would you like to make\"),\n",
    "    (\"do you deliver\", \"yes, we deliver within a 5 mile radius\"),\n",
    "    (\"what payment methods do you accept\", \"we accept cash, credit cards, and mobile payments\"),\n",
    "    (\"is there a parking facility\", \"yes, free parking is available behind the restaurant\"),\n",
    "    (\"thank you very much\", \"you're welcome, happy to help\"),\n",
    "    (\"I have a food allergy\", \"please let us know your allergy, and we will accommodate\"),\n",
    "    (\"can I book a table\", \"yes, for how many people and what time\"),\n",
    "    (\"what's your restaurant address\", \"we are located at 123 Main Street\"),\n",
    "    (\"do you have vegan meals\", \"yes, we offer delicious vegan options\"),\n",
    "    (\"can I get nutritional information\", \"nutritional info is available on our website\"),\n",
    "    (\"how long is the wait time\", \"usually about 15 minutes during peak hours\"),\n",
    "    (\"do you have a kids menu\", \"yes, we have a special menu for children\"),\n",
    "    (\"can I cancel my order\", \"please provide your order number to cancel\"),\n",
    "    (\"what are your opening hours\", \"we are open from 9 am to 10 pm daily\"),\n",
    "    (\"is takeout available\", \"yes, you can order takeout anytime during opening hours\"),        \n",
    "]\n",
    "\n",
    "# Build vocabulary\n",
    "all_texts = [t[0] + \" \" + t[1] for t in train_data]\n",
    "all_words = set(word for sentence in all_texts for word in sentence.lower().split())\n",
    "word2idx = {w: i + 1 for i, w in enumerate(sorted(all_words))}\n",
    "idx2word = np.array(['<pad>'] + sorted(all_words))\n",
    "vocab_size = len(idx2word)\n",
    "\n",
    "max_input_len = 6\n",
    "max_resp_len = 8\n",
    "\n",
    "def encode_sentence(sent, max_len):\n",
    "    words = sent.lower().split()\n",
    "    seq = [word2idx.get(w, 0) for w in words]\n",
    "    seq = seq[:max_len] + [0] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "X_input = np.array([encode_sentence(t[0], max_input_len) for t in train_data])\n",
    "X_resp_in = np.array([encode_sentence(t[1], max_resp_len) for t in train_data])\n",
    "X_resp_out = np.array([encode_sentence(t[1], max_resp_len)[1:] + [0] for t in train_data]) # shifted\n",
    "\n",
    "class Expert(Model):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense2 = Dense(d_model)\n",
    "    def call(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "class GatingNetwork(Model):\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super().__init__()\n",
    "        self.dense = Dense(num_experts)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "    def call(self, x):\n",
    "        logits = self.dense(x)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "class MoEResponseGenerator(Model):\n",
    "    def __init__(self, vocab_size, d_model, num_experts, max_resp_len, lstm_units=128):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = [Expert(d_model) for _ in range(num_experts)]\n",
    "        self.gating_net = GatingNetwork(d_model, num_experts)\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        self.to_h = Dense(lstm_units)\n",
    "        self.to_c = Dense(lstm_units)\n",
    "        self.output_layer = Dense(vocab_size)\n",
    "        self.max_resp_len = max_resp_len\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_seq, resp_in_seq = inputs\n",
    "        enc_emb = self.embedding(input_seq)\n",
    "        pooled = self.global_pool(enc_emb)\n",
    "\n",
    "        gating_probs = self.gating_net(pooled)\n",
    "        expert_outs = tf.stack([expert(pooled) for expert in self.experts], axis=1)\n",
    "        gated_rep = tf.reduce_sum(tf.expand_dims(gating_probs, 2) * expert_outs, axis=1)\n",
    "\n",
    "        h_state = self.to_h(gated_rep)\n",
    "        c_state = self.to_c(gated_rep)\n",
    "\n",
    "        resp_emb = self.embedding(resp_in_seq)\n",
    "        lstm_out, _, _ = self.lstm(resp_emb, initial_state=[h_state, c_state])\n",
    "\n",
    "        logits = self.output_layer(lstm_out)\n",
    "        return logits, gating_probs\n",
    "\n",
    "# Hyperparams and dataset\n",
    "d_model = 32\n",
    "num_experts = 4\n",
    "batch_size = 2\n",
    "epochs = 250\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((X_input, X_resp_in), X_resp_out)).shuffle(20).batch(batch_size)\n",
    "\n",
    "model = MoEResponseGenerator(vocab_size, d_model, num_experts, max_resp_len)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, _ = model(inputs, training=True)\n",
    "        loss = loss_fn(labels, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for in_batch, out_batch in dataset:\n",
    "        loss = train_step(in_batch, out_batch)\n",
    "        total_loss += loss.numpy()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Loss: {total_loss/len(dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_logits(logits, temperature=1.0, top_k=5):\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        values, _ = tf.math.top_k(logits, k=top_k)\n",
    "        min_values = values[:, -1, None]\n",
    "        logits = tf.where(\n",
    "            logits < min_values,\n",
    "            tf.fill(tf.shape(logits), float('-inf')),\n",
    "            logits,\n",
    "        )\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    next_token = tf.random.categorical(tf.math.log(probabilities), num_samples=1)\n",
    "    return tf.squeeze(next_token, axis=-1).numpy()\n",
    "\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    input_seq = np.array([encode_sentence(input_text, max_input_len)])\n",
    "    response_seq = np.zeros((1, max_resp_len), dtype=np.int32)\n",
    "    generated_tokens = []\n",
    "    gating_probs = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        logits, gating_probs = model((input_seq, response_seq), training=False)\n",
    "        logits_step = logits[:, i % max_resp_len, :]\n",
    "        \n",
    "        next_token = sample_from_logits(logits_step, temperature=temperature, top_k=top_k)[0]\n",
    "\n",
    "        if next_token == 0:  # end on padding token\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(idx2word[next_token])\n",
    "        if i + 1 < max_resp_len:\n",
    "            response_seq[0, i] = next_token\n",
    "        else:\n",
    "            response_seq = np.roll(response_seq, -1, axis=1)\n",
    "            response_seq[0, -1] = next_token\n",
    "\n",
    "    top_expert = np.argmax(gating_probs[0].numpy()) if gating_probs is not None else -1\n",
    "    return \" \".join(generated_tokens), top_expert, gating_probs[0].numpy()\n",
    "\n",
    "\n",
    "# Demo\n",
    "inference_prompts = [\n",
    "    \"hello there\",\n",
    "    \"I want to order pizza\",\n",
    "    \"goodbye\",\n",
    "    \"can you help me order food\",\n",
    "    \"hi, what's going on?\",\n",
    "    \"will it rain tomorrow?\",\n",
    "    \"how do I say goodbye politely?\",\n",
    "    \"what toppings do you have?\",\n",
    "    \"is today sunny or cloudy?\",\n",
    "    \"see you soon\"\n",
    "]\n",
    "for input_text in inference_prompts:\n",
    "    response, expert_used, gating_distribution = generate_response(model, input_text, max_len=500)\n",
    "\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Generated response:\", response)\n",
    "    print(\"Top expert used:\", expert_used)\n",
    "    print(\"Gating probabilities:\", np.round(gating_distribution, 3))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_array = np.array(image)\n",
    "    mean_colors = image_array.mean(axis=(0, 1))\n",
    "    color_names = ['red', 'green', 'blue']\n",
    "    dominant_color = color_names[np.argmax(mean_colors)]\n",
    "    return f\"Dominant color is {dominant_color}.\"\n",
    "\n",
    "def multimodal_response(model, input_text=None, image_path=None):\n",
    "    result = \"\"\n",
    "    if input_text:\n",
    "        # Use your generate_response logic for text\n",
    "        response, exp_used, gating_distribution = generate_response(model, input_text, max_len=30)\n",
    "        result += f\"Text response: {response}\\nTop expert used: {exp_used}\\n\"\n",
    "    if image_path:\n",
    "        # Classify the image\n",
    "        image_result = classify_image(image_path)\n",
    "        result += f\"Image analysis: {image_result}\\n\"\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "print(multimodal_response(model, input_text=\"what toppings do you have?\", image_path=\"sample_image.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Softmax, LSTM, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# ----------- MNIST DIGIT CLASSIFIER -----------\n",
    "\n",
    "class DigitClassifier(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(32, (3,3), activation='relu')\n",
    "        self.pool1 = MaxPooling2D((2,2))\n",
    "        self.conv2 = Conv2D(64, (3,3), activation='relu')\n",
    "        self.pool2 = MaxPooling2D((2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(64, activation='relu')\n",
    "        self.out = Dense(10)  # digits 0-9 logits\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return self.out(x)\n",
    "\n",
    "def prepare_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# Train or load a pretrained digit classifier\n",
    "digit_classifier = DigitClassifier()\n",
    "(x_train, y_train), (x_test, y_test) = prepare_mnist_data()\n",
    "digit_classifier.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "print(\"Training digit classifier (this may take a bit)...\")\n",
    "digit_classifier.fit(x_train, y_train, epochs=3, batch_size=128, validation_split=0.1)\n",
    "\n",
    "# ----------- YOUR MOE TEXT GENERATION MODEL (copy your existing) -----------\n",
    "\n",
    "# (Paste your current MoEResponseGenerator, Expert, GatingNetwork, encode_sentence, sample_from_logits, etc. here)\n",
    "# For brevity, let's assume it's loaded as `moe_model`\n",
    "\n",
    "# For this example, just a placeholder generate_response:\n",
    "def generate_response(model, input_text, max_len=30, temperature=1.0, top_k=5):\n",
    "    # Use your real model's generation logic here\n",
    "    return f\"Simulated response to: {input_text}\", 1, np.array([0.1, 0.7, 0.1, 0.1])\n",
    "\n",
    "# ----------- MNIST INCREMENT LOGIC -----------\n",
    "\n",
    "def get_incremented_digit_image(input_image, x_dataset, y_dataset):\n",
    "    # Normalize pixel values and ensure float32\n",
    "    img = input_image.astype('float32')\n",
    "    if img.max() > 1.0:\n",
    "        img /= 255.0\n",
    "\n",
    "    # Add channel dimension if missing\n",
    "    if len(img.shape) == 2:   # grayscale image shape (28,28)\n",
    "        img = np.expand_dims(img, axis=-1)  # become (28,28,1)\n",
    "\n",
    "    # Add batch dimension for model input\n",
    "    img = np.expand_dims(img, axis=0)  # become (1,28,28,1)\n",
    "\n",
    "    logits = digit_classifier(img)\n",
    "    pred_digit = tf.argmax(logits, axis=1).numpy()[0]\n",
    "\n",
    "    inc_digit = 0 if pred_digit == 9 else pred_digit + 1\n",
    "    idx = np.where(y_dataset == inc_digit)[0][0]\n",
    "    inc_image = x_dataset[idx][:, :, 0]  # remove channel for display\n",
    "\n",
    "    return pred_digit, inc_digit, inc_image\n",
    "\n",
    "\n",
    "# ----------- MULTIMODAL HANDLER -----------\n",
    "\n",
    "def multimodal_handler(input_text=None, input_image=None):\n",
    "    if input_text:\n",
    "        response, expert, gating = generate_response(None, input_text)\n",
    "        print(f\"Text Input: {input_text}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Top expert used: {expert}\")\n",
    "        print(f\"Gating probabilities: {gating}\")\n",
    "    if input_image is not None:\n",
    "        pred_digit, inc_digit, inc_image = get_incremented_digit_image(input_image, x_test, np.argmax(y_test, axis=1))\n",
    "        print(f\"Input Digit: {pred_digit}, Incremented Digit: {inc_digit}\")\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title(f\"Input: {pred_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(inc_image, cmap='gray')\n",
    "        plt.title(f\"Output: {inc_digit}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# ----------- DEMO -----------\n",
    "\n",
    "# Text-only\n",
    "multimodal_handler(input_text=\"I need help with my order.\")\n",
    "\n",
    "# Image-only\n",
    "multimodal_handler(input_image=x_test[5])\n",
    "\n",
    "# Both modalities\n",
    "multimodal_handler(input_text=\"What digit is this?\", input_image=x_test[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_3.10_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
